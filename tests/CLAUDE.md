# Testing Guidelines

## ğŸ§ª TESTING PHILOSOPHY

- **Test behavior, not implementation**
- **Write tests BEFORE fixing bugs**
- **Each test should test ONE thing**
- **Test names should describe what they test**

## ğŸ“ TEST STRUCTURE

```
tests/
â”œâ”€â”€ unit/              # <15s total (74 files), no external deps, comprehensive mocking
â”‚   â”œâ”€â”€ api/           # API models, validation, business logic
â”‚   â”œâ”€â”€ cli/           # CLI command parsing, validation, logic  
â”‚   â”œâ”€â”€ config/        # Configuration loading and validation
â”‚   â”œâ”€â”€ core/          # System fundamentals, metadata
â”‚   â”œâ”€â”€ data/          # Data adapters, transformations
â”‚   â”œâ”€â”€ fuzzy/         # Fuzzy logic calculations and algorithms
â”‚   â”œâ”€â”€ ib/            # IB connection logic, error handling
â”‚   â”œâ”€â”€ indicators/    # Technical indicator calculations
â”‚   â”œâ”€â”€ neural/        # Neural network foundations
â”‚   â”œâ”€â”€ training/      # Model storage, processors
â”‚   â”œâ”€â”€ utils/         # Utility functions and helpers
â”‚   â””â”€â”€ visualization/ # Chart generation logic
â”œâ”€â”€ integration/       # <30s total (18 files), component interactions, mocked externals
â”‚   â”œâ”€â”€ api/           # HTTP endpoint integration
â”‚   â”œâ”€â”€ cli/           # CLI command integration
â”‚   â”œâ”€â”€ data_pipeline/ # Data flow integration
â”‚   â”œâ”€â”€ fuzzy/         # Multi-timeframe fuzzy integration
â”‚   â”œâ”€â”€ host_services/ # Service orchestration with mocked externals
â”‚   â”œâ”€â”€ ib/            # Complex IB parsing integration
â”‚   â”œâ”€â”€ services/      # Service orchestration
â”‚   â”œâ”€â”€ visualization/ # End-to-end chart generation
â”‚   â””â”€â”€ workflows/     # Decision orchestration, backtesting
â”œâ”€â”€ e2e/               # <5min total, full system tests
â”‚   â”œâ”€â”€ container/     # Container-based system tests
â”‚   â””â”€â”€ real/          # Real service integration
â”œâ”€â”€ host_service/      # Tests requiring real host services (manual only)
â”‚   â”œâ”€â”€ ib_integration/     # Real IB Gateway/TWS tests
â”‚   â””â”€â”€ training_service/   # Real training service tests
â””â”€â”€ manual/            # Manual/nightly tests only
    â”œâ”€â”€ performance/   # Performance benchmarks
    â”œâ”€â”€ stress/        # Stress testing
    â””â”€â”€ real_trading/  # Real market tests
```

## ğŸƒ RUNNING TESTS

### Standard Commands (Use Makefile)

```bash
# Fast development loop - run on every change
make test-unit          # Unit tests only (<15s) - DEFAULT CHOICE
make test-fast          # Alias for test-unit

# Integration testing - run when testing component interactions  
make test-integration   # Integration tests (<30s)

# Full system testing - run before major commits
make test-e2e          # End-to-end tests (<5min)

# Host service testing - requires real services running
make test-host         # Host service tests (IB Gateway, training services)

# Coverage and reporting
make test-coverage     # Unit tests with HTML coverage report
make test-all          # All tests (unit + integration + e2e)

# Performance testing
make test-performance  # Performance benchmarks (manual category)

# Code quality - run before committing
make quality           # Lint + format + typecheck
make lint              # Ruff linting only  
make format            # Black formatting only
make typecheck         # MyPy type checking only

# CI simulation - matches GitHub Actions
make ci                # Run unit tests + quality checks
```

### Direct Pytest Commands

```bash
# Unit tests (fastest feedback)
uv run pytest tests/unit/ -v

# Specific test categories
uv run pytest tests/unit/indicators/ -v     # Specific module
uv run pytest tests/integration/api/ -v     # Integration tests
uv run pytest tests/e2e/container/ -v       # Container E2E tests

# With markers
uv run pytest -m "unit" -v                  # Only unit tests
uv run pytest -m "integration" -v           # Only integration tests  
uv run pytest -m "host_service" -v          # Only host service tests

# Coverage reporting
uv run pytest tests/unit/ --cov=ktrdr --cov-report=html
uv run pytest --cov=ktrdr --cov-report=term-missing
```

## ğŸš« TESTING ANTI-PATTERNS

âŒ Tests that depend on test order
âœ… Each test independent

âŒ Tests that use real external services
âœ… Mock external dependencies

âŒ Tests with no assertions
âœ… Always assert expected behavior

âŒ Commenting out failing tests
âœ… Fix or properly skip with reason

## ğŸ“ TEST PATTERNS

### Arrange-Act-Assert
```python
def test_indicator_calculation():
    # Arrange
    data = create_test_data()
    indicator = RSI(period=14)
    
    # Act
    result = indicator.calculate(data)
    
    # Assert
    assert len(result) == len(data)
    assert result.iloc[-1] == pytest.approx(65.4, rel=0.01)
```

### Parameterized Tests
```python
@pytest.mark.parametrize("period,expected", [
    (14, 65.4),
    (21, 58.2),
    (7, 72.1),
])
def test_rsi_periods(period, expected):
    # Test multiple cases efficiently
```

## ğŸ”§ FIXTURES

Common fixtures in `conftest.py`:
- `sample_ohlcv_data` - Standard price data
- `mock_ib_connection` - Mocked IB client
- `test_config` - Test configuration

## ğŸš€ CI/CD PIPELINE

### GitHub Actions Workflow

The CI pipeline runs automatically on pushes and pull requests:

#### Default Run (Every Push)
- **unit-tests-and-quality**: Unit tests + code quality checks
  - Runs `make test-coverage` (unit tests with coverage)
  - Runs `make quality` (lint + format + typecheck) 
  - Uploads coverage to Codecov
  - **Performance**: <30 seconds total

#### Manual Trigger (workflow_dispatch)
- **integration-tests**: Component interaction tests  
  - Runs `make test-integration`
  - Uses mocked external services
  - **Performance**: <45 seconds total
  
- **e2e-tests**: Full system tests
  - Starts Docker containers
  - Runs `make test-e2e` 
  - Tests container-based system workflows
  - **Performance**: <8 minutes total

### Local Testing Before Commit

```bash
# Essential pre-commit checks (< 30 seconds)
make test-unit          # Unit tests - MUST PASS
make quality           # Code quality - MUST PASS

# Optional comprehensive testing
make test-integration  # Integration tests
make test-coverage     # Coverage report
```

### Host Service Tests (Manual Only)

Tests requiring real services are NOT run in CI:

```bash
# Real IB Gateway tests (requires IB Gateway/TWS running)
make test-host         # All host service tests
uv run pytest tests/host_service/ib_integration/ -v

# Real training service tests (requires training service running)  
uv run pytest tests/host_service/training_service/ -v

# Real E2E tests (requires real external services)
uv run pytest tests/e2e/real/ -v
```

### Performance Standards

| Test Category | Target Time | Current Performance | Status |
|---------------|-------------|-------------------|---------|
| Unit Tests | <15s | 8.89s | âœ… EXCELLENT |
| Integration | <30s | 6.67s (fast subset) | âœ… EXCELLENT |
| E2E Container | <5min | ~8min (with startup) | âš ï¸ ACCEPTABLE |
| Host Service | Manual | N/A (skip in CI) | â­ï¸ MANUAL |

### Coverage Targets

- **Unit test coverage**: >30% (current: 32%)
- **Core modules**: >85% (indicators: 87-100%, metadata: 89%)  
- **Coverage reporting**: Codecov integration active
- **HTML reports**: Generated locally with `make test-coverage`

## âš ï¸ HOST SERVICE TESTS

Located in `tests/host_service/`:
- **Require real services**: IB Gateway/TWS, training services
- **Not run in CI**: Manual execution only
- **Usage**: `make test-host` or direct pytest commands
- **Requirements**: Document service setup clearly