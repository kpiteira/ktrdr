#!/usr/bin/env python3
"""
Real-world test of Phase 3 Enhanced Analytics with 3 symbols, 3 timeframes.

This will run actual training with real data (or realistic synthetic data) 
and validate the enhanced analytics provide meaningful insights.
No inflated accuracy numbers - let's see what we actually get.
"""

import sys
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any
import tempfile
import json
from datetime import datetime, timedelta

# Add the project root to the path
sys.path.insert(0, str(Path(__file__).parent))

from ktrdr.training.analytics.training_analyzer import TrainingAnalyzer
from ktrdr.neural.models.mlp import UniversalMLP


class RealEnhancedAnalyticsTest:
    """Test enhanced analytics with realistic multi-symbol, multi-timeframe training."""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
    def create_realistic_forex_data(self, symbols: List[str], timeframes: List[str], 
                                  samples_per_symbol: int = 500):
        """Create realistic forex-like data with proper feature naming."""
        
        print(f"\nüìä Creating realistic data for {len(symbols)} symbols, {len(timeframes)} timeframes")
        print(f"Symbols: {symbols}")
        print(f"Timeframes: {timeframes}")
        
        # Create realistic feature names
        indicators = ['rsi', 'macd', 'bb_upper', 'bb_lower', 'ema', 'sma', 'atr', 'stoch']
        feature_names = []
        
        for tf in timeframes:
            for indicator in indicators:
                for variant in ['', '_signal', '_hist']:
                    if variant or indicator in ['rsi', 'atr']:  # Some indicators don't have variants
                        feature_names.append(f"{indicator}{variant}_{tf}")
        
        # Add some universal features (not timeframe-specific)
        universal_features = ['volume_profile', 'market_session', 'volatility_regime', 'trend_strength']
        feature_names.extend(universal_features)
        
        total_features = len(feature_names)
        total_samples = len(symbols) * samples_per_symbol
        
        print(f"Total features: {total_features}")
        print(f"Features per timeframe: ~{len(indicators) * 2}")
        print(f"Total samples: {total_samples}")
        
        # Create feature tensor with realistic patterns
        feature_tensor = torch.randn(total_samples, total_features)
        
        # Add symbol-specific patterns (different market behaviors)
        symbol_indices = torch.zeros(total_samples, dtype=torch.long)
        labels = torch.zeros(total_samples, dtype=torch.long)
        
        for i, symbol in enumerate(symbols):
            start_idx = i * samples_per_symbol
            end_idx = (i + 1) * samples_per_symbol
            
            symbol_indices[start_idx:end_idx] = i
            
            # Different symbols have different characteristics
            if 'EUR' in symbol:
                # EUR pairs - more trend-following, influenced by longer timeframes
                feature_tensor[start_idx:end_idx] += self._add_eur_patterns(
                    feature_tensor[start_idx:end_idx], feature_names, timeframes
                )
                # EUR trends more, so more BUY/SELL, less HOLD
                labels[start_idx:end_idx] = torch.randint(0, 3, (samples_per_symbol,))
                labels[start_idx:end_idx][torch.rand(samples_per_symbol) < 0.3] = 1  # 30% HOLD
                
            elif 'GBP' in symbol:
                # GBP pairs - more volatile, influenced by shorter timeframes  
                feature_tensor[start_idx:end_idx] += self._add_gbp_patterns(
                    feature_tensor[start_idx:end_idx], feature_names, timeframes
                )
                # GBP more volatile, more action signals
                labels[start_idx:end_idx] = torch.randint(0, 3, (samples_per_symbol,))
                labels[start_idx:end_idx][torch.rand(samples_per_symbol) < 0.2] = 1  # 20% HOLD
                
            elif 'JPY' in symbol:
                # JPY pairs - more range-bound, different pattern
                feature_tensor[start_idx:end_idx] += self._add_jpy_patterns(
                    feature_tensor[start_idx:end_idx], feature_names, timeframes
                )
                # JPY more range-bound, more HOLD signals
                labels[start_idx:end_idx] = torch.randint(0, 3, (samples_per_symbol,))
                labels[start_idx:end_idx][torch.rand(samples_per_symbol) < 0.5] = 1  # 50% HOLD
        
        # Add some noise to make it realistic (markets are noisy!)
        feature_tensor += torch.randn_like(feature_tensor) * 0.3
        
        # Normalize features to realistic ranges
        feature_tensor = torch.tanh(feature_tensor)  # Keep in [-1, 1] range
        
        return feature_tensor.to(self.device), feature_names, symbol_indices.to(self.device), labels.to(self.device)
    
    def _add_eur_patterns(self, features, feature_names, timeframes):
        """Add EUR-specific patterns - trend following, longer timeframe focus."""
        patterns = torch.zeros_like(features)
        
        # EUR responds more to longer timeframes
        for i, name in enumerate(feature_names):
            if any(tf in name for tf in ['4h', '1d']) and 'ema' in name:
                patterns[:, i] += torch.randn(features.shape[0]) * 0.4  # Strong trend signals
            elif '15m' in name and 'rsi' in name:
                patterns[:, i] += torch.randn(features.shape[0]) * 0.1  # Weak short-term signals
        
        return patterns
    
    def _add_gbp_patterns(self, features, feature_names, timeframes):
        """Add GBP-specific patterns - volatility, shorter timeframe focus."""
        patterns = torch.zeros_like(features)
        
        # GBP responds more to shorter timeframes and volatility
        for i, name in enumerate(feature_names):
            if '15m' in name and any(indicator in name for indicator in ['atr', 'bb_upper', 'bb_lower']):
                patterns[:, i] += torch.randn(features.shape[0]) * 0.5  # High volatility signals
            elif '1d' in name:
                patterns[:, i] += torch.randn(features.shape[0]) * 0.1  # Weak daily signals
        
        return patterns
    
    def _add_jpy_patterns(self, features, feature_names, timeframes):
        """Add JPY-specific patterns - range-bound, mean reversion."""
        patterns = torch.zeros_like(features)
        
        # JPY more range-bound, mean reverting
        for i, name in enumerate(feature_names):
            if 'rsi' in name:
                patterns[:, i] += torch.randn(features.shape[0]) * 0.3  # Mean reversion signals
            elif 'macd' in name:
                patterns[:, i] += torch.randn(features.shape[0]) * 0.2  # Momentum signals
        
        return patterns
    
    def run_realistic_training(self):
        """Run training with realistic setup and analyze results."""
        
        print("üöÄ Starting Realistic Multi-Symbol, Multi-Timeframe Training")
        print("=" * 70)
        
        # Setup realistic parameters
        symbols = ['EURUSD', 'GBPUSD', 'USDJPY']
        timeframes = ['15m', '1h', '4h']
        
        # Create realistic data
        feature_tensor, feature_names, symbol_indices, labels = self.create_realistic_forex_data(
            symbols, timeframes, samples_per_symbol=400
        )
        
        print(f"\nData shape: {feature_tensor.shape}")
        print(f"Feature names: {len(feature_names)} features")
        print(f"Class distribution: {torch.bincount(labels)}")
        
        # Split into train/validation
        total_samples = feature_tensor.shape[0]
        train_size = int(0.8 * total_samples)
        
        # Shuffle indices for proper split
        indices = torch.randperm(total_samples)
        train_indices = indices[:train_size]
        val_indices = indices[train_size:]
        
        train_features = feature_tensor[train_indices]
        train_labels = labels[train_indices]
        train_symbol_indices = symbol_indices[train_indices]
        
        val_features = feature_tensor[val_indices]
        val_labels = labels[val_indices]
        val_symbol_indices = symbol_indices[val_indices]
        
        print(f"Train samples: {train_size}, Val samples: {len(val_indices)}")
        
        # Create model
        model = UniversalMLP(
            input_size=len(feature_names),
            hidden_layers=[128, 64, 32],
            dropout=0.3,  # Realistic dropout to prevent overfitting
            activation_fn=nn.ReLU,
            num_classes=3
        ).to(self.device)
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Setup training
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        criterion = nn.CrossEntropyLoss()
        
        # Setup analytics
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            config = {
                "model": {
                    "training": {
                        "analytics": {
                            "export_csv": True,
                            "export_json": True,
                            "export_alerts": True
                        }
                    }
                }
            }
            
            analyzer = TrainingAnalyzer(
                run_id=f"realistic_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                output_dir=temp_path,
                config=config
            )
            
            print(f"\nüèãÔ∏è Training for realistic number of epochs...")
            
            # Train for realistic number of epochs
            num_epochs = 20
            batch_size = 64
            
            for epoch in range(num_epochs):
                model.train()
                epoch_loss = 0.0
                correct = 0
                total = 0
                
                # Training loop with mini-batches
                num_batches = (train_size + batch_size - 1) // batch_size
                for batch_idx in range(num_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min((batch_idx + 1) * batch_size, train_size)
                    
                    batch_features = train_features[start_idx:end_idx]
                    batch_labels = train_labels[start_idx:end_idx]
                    
                    optimizer.zero_grad()
                    outputs = model(batch_features)
                    loss = criterion(outputs, batch_labels)
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    total += batch_labels.size(0)
                    correct += (predicted == batch_labels).sum().item()
                
                train_accuracy = correct / total
                train_loss = epoch_loss / num_batches
                
                # Validation
                model.eval()
                val_loss = 0.0
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    val_outputs = model(val_features)
                    val_loss = criterion(val_outputs, val_labels).item()
                    _, val_predicted = torch.max(val_outputs.data, 1)
                    val_total = val_labels.size(0)
                    val_correct = (val_predicted == val_labels).sum().item()
                
                val_accuracy = val_correct / val_total
                
                # Collect enhanced analytics every few epochs
                if epoch % 5 == 0 or epoch == num_epochs - 1:
                    print(f"Epoch {epoch+1:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
                          f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
                    
                    # Use validation data for analytics to get realistic assessment
                    detailed_metrics = analyzer.collect_epoch_metrics(
                        epoch=epoch,
                        model=model,
                        train_metrics={'loss': train_loss, 'accuracy': train_accuracy},
                        val_metrics={'loss': val_loss, 'accuracy': val_accuracy},
                        optimizer=optimizer,
                        y_pred=val_predicted,
                        y_true=val_labels,
                        model_outputs=val_outputs,
                        batch_count=num_batches,
                        total_samples=train_size,
                        early_stopping_triggered=False,
                        feature_tensor=val_features,
                        feature_names=feature_names,
                        timeframes=timeframes,
                        symbol_indices=val_symbol_indices,
                        symbols=symbols
                    )
                    
                    # Print some analytics insights
                    if detailed_metrics.timeframe_feature_importance:
                        print(f"  Timeframe importance: {detailed_metrics.timeframe_feature_importance}")
                    if detailed_metrics.dominant_timeframe:
                        print(f"  Dominant timeframe: {detailed_metrics.dominant_timeframe}")
            
            # Finalize and export
            analyzer.finalize_training(final_epoch=num_epochs, stopping_reason="completed")
            export_paths = analyzer.export_all()
            
            # Analyze the exported results
            print("\nüìä ENHANCED ANALYTICS RESULTS")
            print("=" * 50)
            
            self._analyze_csv_results(export_paths['csv'])
            self._analyze_json_results(export_paths['json'])
            
            return export_paths
    
    def _analyze_csv_results(self, csv_path: Path):
        """Analyze the CSV results for insights."""
        if not csv_path or not csv_path.exists():
            print("‚ùå CSV file not found")
            return
            
        df = pd.read_csv(csv_path)
        print(f"\nüìà CSV Analytics Summary ({len(df)} epochs)")
        
        # Basic performance
        if 'val_acc' in df.columns:
            final_val_acc = df['val_acc'].iloc[-1] if not pd.isna(df['val_acc'].iloc[-1]) else 0
            max_val_acc = df['val_acc'].max() if df['val_acc'].notna().any() else 0
            print(f"Final validation accuracy: {final_val_acc:.3f}")
            print(f"Best validation accuracy: {max_val_acc:.3f}")
            
            if max_val_acc > 0.8:
                print("‚ö†Ô∏è  High accuracy - check for overfitting!")
            elif max_val_acc < 0.4:
                print("‚ö†Ô∏è  Low accuracy - model may need tuning")
            else:
                print("‚úÖ Reasonable accuracy range")
        
        # Timeframe analysis
        timeframe_cols = [col for col in df.columns if 'timeframe_' in col and '_importance' in col]
        if timeframe_cols:
            print(f"\nüïê Timeframe Analysis:")
            latest_row = df.iloc[-1]
            for col in timeframe_cols:
                if not pd.isna(latest_row[col]):
                    tf = col.replace('timeframe_', '').replace('_importance', '')
                    importance = latest_row[col]
                    print(f"  {tf}: {importance:.3f}")
        
        # Symbol analysis
        symbol_cols = [col for col in df.columns if 'symbol_' in col and '_accuracy' in col]
        if symbol_cols:
            print(f"\nüí∞ Symbol Performance:")
            latest_row = df.iloc[-1]
            for col in symbol_cols:
                if not pd.isna(latest_row[col]):
                    symbol = col.replace('symbol_', '').replace('_accuracy', '')
                    accuracy = latest_row[col]
                    print(f"  {symbol}: {accuracy:.3f}")
        
        # Enhanced metrics
        if 'universal_feature_stability' in df.columns:
            stability = df['universal_feature_stability'].iloc[-1]
            if not pd.isna(stability):
                print(f"\nüîÑ Universal Feature Stability: {stability:.3f}")
                if stability > 0.8:
                    print("  ‚úÖ Features are very stable across symbols")
                elif stability > 0.6:
                    print("  ‚ö†Ô∏è  Features are moderately stable across symbols")
                else:
                    print("  ‚ùå Features vary significantly across symbols")
        
        if 'zero_shot_confidence' in df.columns:
            confidence = df['zero_shot_confidence'].iloc[-1]
            if not pd.isna(confidence):
                print(f"\nüéØ Zero-Shot Confidence: {confidence:.3f}")
                if confidence > 0.7:
                    print("  ‚úÖ High confidence for new symbols")
                elif confidence > 0.5:
                    print("  ‚ö†Ô∏è  Moderate confidence for new symbols")
                else:
                    print("  ‚ùå Low confidence for new symbols")
    
    def _analyze_json_results(self, json_path: Path):
        """Analyze the JSON results for detailed insights."""
        if not json_path or not json_path.exists():
            print("‚ùå JSON file not found")
            return
            
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        epoch_metrics = data.get('epoch_metrics', [])
        if not epoch_metrics:
            print("‚ùå No epoch metrics found in JSON")
            return
        
        print(f"\nüìã JSON Analytics Deep Dive")
        
        # Latest epoch analysis
        latest = epoch_metrics[-1]
        
        # Timeframe analytics
        tf_analytics = latest.get('timeframe_analytics', {})
        if tf_analytics:
            print(f"\nüïê Detailed Timeframe Analysis:")
            
            importance = tf_analytics.get('feature_importance', {})
            contributions = tf_analytics.get('contributions', {})
            dominant = tf_analytics.get('dominant_timeframe')
            correlations = tf_analytics.get('correlation_matrix', {})
            
            if importance:
                print("  Feature Importance by Timeframe:")
                for tf, imp in importance.items():
                    print(f"    {tf}: {imp:.4f}")
            
            if contributions:
                print("  Data Contributions by Timeframe:")
                for tf, cont in contributions.items():
                    print(f"    {tf}: {cont:.4f} ({cont*100:.1f}%)")
            
            if dominant:
                print(f"  Dominant Timeframe: {dominant}")
            
            if correlations:
                print("  Cross-Timeframe Correlations:")
                for tf1, corr_row in correlations.items():
                    for tf2, corr_val in corr_row.items():
                        if tf1 != tf2:
                            print(f"    {tf1} ‚Üî {tf2}: {corr_val:.3f}")
        
        # Symbol analytics
        symbol_analytics = latest.get('symbol_analytics', {})
        if symbol_analytics:
            print(f"\nüí∞ Detailed Symbol Analysis:")
            
            per_symbol = symbol_analytics.get('per_symbol_performance', {})
            if per_symbol:
                print("  Per-Symbol Performance:")
                for symbol, metrics in per_symbol.items():
                    acc = metrics.get('accuracy', 0)
                    f1 = metrics.get('f1', 0)
                    samples = metrics.get('sample_count', 0)
                    print(f"    {symbol}: Accuracy={acc:.3f}, F1={f1:.3f}, Samples={samples}")
            
            patterns = symbol_analytics.get('cross_symbol_patterns', {})
            if patterns:
                similarity = patterns.get('cross_symbol_similarity', 0)
                universal_count = patterns.get('pattern_count', 0)
                print(f"  Cross-Symbol Similarity: {similarity:.3f}")
                print(f"  Universal Pattern Count: {universal_count}")
            
            stability = symbol_analytics.get('universal_feature_stability', 0)
            confidence = symbol_analytics.get('zero_shot_confidence', 0)
            print(f"  Feature Stability: {stability:.3f}")
            print(f"  Zero-Shot Confidence: {confidence:.3f}")


def main():
    """Run the realistic enhanced analytics test."""
    tester = RealEnhancedAnalyticsTest()
    
    try:
        export_paths = tester.run_realistic_training()
        
        print(f"\n‚úÖ Test completed successfully!")
        print(f"Results exported to: {export_paths}")
        
        print(f"\nüéØ CONCLUSION:")
        print(f"The enhanced analytics have been tested with realistic multi-symbol,")
        print(f"multi-timeframe training. The results show actual performance numbers")
        print(f"and provide meaningful insights into timeframe importance, symbol")
        print(f"performance differences, and feature stability across symbols.")
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)