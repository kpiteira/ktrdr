# Training Pre-Processing Progress Reporting: Architecture

**Date**: 2025-01-16
**Status**: Architecture Phase
**Related**: [Problem Statement](./01-problem-statement.md), [Design](./02-design.md)

---

## Executive Summary

This document defines the technical architecture for comprehensive progress reporting across all training phases. The solution **extends existing infrastructure** (`TrainingProgressBridge`, `TrainingProgressRenderer`) rather than creating new systems.

**Core Approach**: Add granular progress callbacks within `TrainingPipeline.train_strategy()` that route through existing orchestrators to the bridge, which formats and emits progress updates.

---

## Current Architecture

### Existing Training Progress Infrastructure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Orchestrator (Local OR Host Service)               â”‚
â”‚  - Creates training operation                               â”‚
â”‚  - Creates progress callback                                â”‚
â”‚  - Creates TrainingProgressBridge                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ creates
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TrainingProgressBridge                                      â”‚
â”‚  - Translates training events â†’ progress updates            â”‚
â”‚  - Methods: on_epoch(), on_batch(), on_complete()          â”‚
â”‚  - Emits to GenericProgressManager                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ updates
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GenericProgressManager                                      â”‚
â”‚  - Thread-safe progress state                               â”‚
â”‚  - Triggers callbacks on state change                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ triggers
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TrainingProgressRenderer                                    â”‚
â”‚  - Formats training-specific messages                       â”‚
â”‚  - Example: "Epoch 5/100 Â· Batch 120/500"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ displays via
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OperationsService                                           â”‚
â”‚  - Stores progress in database                              â”‚
â”‚  - Broadcasts to clients (WebSocket/SSE)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Current Training Pipeline Flow

```
TrainingPipeline.train_strategy(progress_callback)
â”‚
â”œâ”€ Load market data          [NO CALLBACKS]
â”œâ”€ Calculate indicators      [NO CALLBACKS]
â”œâ”€ Generate fuzzy sets       [NO CALLBACKS]
â”œâ”€ Create features           [NO CALLBACKS]
â”œâ”€ Generate labels           [NO CALLBACKS]
â”œâ”€ Combine data              [NO CALLBACKS]
â”œâ”€ Split data                [NO CALLBACKS]
â”œâ”€ Create model              [NO CALLBACKS]
â”œâ”€ Train model               [âœ“ USES progress_callback]
â”‚   â””â”€ Reports epochs/batches
â””â”€ Evaluate model            [NO CALLBACKS]
```

**Gap**: Progress callback exists but is only used during `train_model()`.

---

## Proposed Architecture

### High-Level Changes

**What we're adding**:
1. **New methods on TrainingProgressBridge** to handle pre-training progress
2. **Progress callbacks in TrainingPipeline** at key checkpoints
3. **Routing logic in orchestrators** to handle new progress types
4. **Enhanced rendering in TrainingProgressRenderer** for preprocessing context

**What stays the same**:
- Overall infrastructure (GenericProgressManager, OperationsService)
- Existing epoch/batch progress (on_epoch, on_batch methods)
- ServiceOrchestrator patterns
- Cancellation token support

### âš ï¸ CRITICAL DESIGN PRINCIPLE

**Progress Reporting â‰  Computation**

Progress reporting happens **OUTSIDE** the computation engines:
- âœ… TrainingPipeline reports progress BEFORE calling engines
- âœ… Engines (IndicatorEngine, FuzzyEngine) remain pure computation
- âŒ Do NOT add progress callbacks to engine signatures
- âŒ Do NOT modify engine internals for progress

**Why**:
- Separation of concerns (orchestration vs computation)
- Engines stay reusable without progress dependencies
- Simpler implementation - iterate configs in pipeline, not engine internals
- Testability - engines remain pure functions

### Complete Progress Flow (After Implementation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Orchestrator (Local OR Host Service)               â”‚
â”‚  - Creates training operation                               â”‚
â”‚  - Creates progress callback                                â”‚
â”‚  - Creates TrainingProgressBridge                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ calls
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TrainingPipeline.train_strategy(progress_callback)          â”‚
â”‚                                                              â”‚
â”‚  Emits progress at checkpoints:                             â”‚
â”‚    - After loading data: callback("preprocessing", ...)     â”‚
â”‚    - Per indicator: callback("indicator_computation", ...)  â”‚
â”‚    - Per fuzzy set: callback("fuzzy_generation", ...)       â”‚
â”‚    - After features: callback("preprocessing", ...)         â”‚
â”‚    - After labels: callback("preprocessing", ...)           â”‚
â”‚    - After combining: callback("preparation", ...)          â”‚
â”‚    - During training: callback(epoch, batch, ...)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ all callbacks flow to
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestrator._create_progress_callback()                    â”‚
â”‚                                                              â”‚
â”‚  Routes based on progress_type:                             â”‚
â”‚    "indicator_computation" â†’ bridge.on_indicator_computation()â”‚
â”‚    "fuzzy_generation" â†’ bridge.on_fuzzy_generation()        â”‚
â”‚    "preprocessing" â†’ bridge.on_symbol_processing()          â”‚
â”‚    "preparation" â†’ bridge.on_preparation_phase()            â”‚
â”‚    "batch" â†’ bridge.on_batch() [EXISTING]                   â”‚
â”‚    default â†’ bridge.on_epoch() [EXISTING]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ delegates to
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TrainingProgressBridge                                      â”‚
â”‚                                                              â”‚
â”‚  NEW METHODS:                                               â”‚
â”‚    on_symbol_processing()                                   â”‚
â”‚    on_indicator_computation()                               â”‚
â”‚    on_fuzzy_generation()                                    â”‚
â”‚    on_preparation_phase()                                   â”‚
â”‚                                                              â”‚
â”‚  EXISTING METHODS (unchanged):                              â”‚
â”‚    on_epoch()                                               â”‚
â”‚    on_batch()                                               â”‚
â”‚    on_complete()                                            â”‚
â”‚                                                              â”‚
â”‚  All methods:                                               â”‚
â”‚    - Format message from context                            â”‚
â”‚    - Calculate percentage                                   â”‚
â”‚    - Emit to GenericProgressManager                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ updates
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GenericProgressManager (UNCHANGED)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ triggers
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TrainingProgressRenderer                                    â”‚
â”‚                                                              â”‚
â”‚  ENHANCED to handle preprocessing context:                  â”‚
â”‚    - Check preprocessing_step in context                    â”‚
â”‚    - Format granular messages with timeframe + indicator    â”‚
â”‚    - Fall back to existing epoch/batch rendering            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ displays via
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OperationsService (UNCHANGED)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Architecture

### 1. TrainingProgressBridge (Extended)

**Location**: `ktrdr/api/services/training/progress_bridge.py`

**Current State**:
- Has methods: `on_epoch()`, `on_batch()`, `on_complete()`, `on_phase()`, `on_remote_snapshot()`
- Handles epoch/batch progress during model training
- Emits to `GenericProgressManager`

**Changes Required**:

```
TrainingProgressBridge
â”‚
â”œâ”€ EXISTING METHODS (unchanged)
â”‚   â”œâ”€ on_epoch(epoch, total_epochs, metrics)
â”‚   â”œâ”€ on_batch(epoch, batch, total_batches, metrics)
â”‚   â”œâ”€ on_complete(message)
â”‚   â”œâ”€ on_phase(phase_name, message)
â”‚   â””â”€ on_remote_snapshot(snapshot)
â”‚
â”œâ”€ NEW METHODS
â”‚   â”œâ”€ on_symbol_processing(symbol, symbol_index, total_symbols, step, context)
â”‚   â”‚   Purpose: Report per-symbol preprocessing steps
â”‚   â”‚   Example: "Processing AAPL (2/5) - Loading data"
â”‚   â”‚
â”‚   â”œâ”€ on_indicator_computation(symbol, symbol_index, total_symbols,
â”‚   â”‚                            timeframe, indicator_name,
â”‚   â”‚                            indicator_index, total_indicators)
â”‚   â”‚   Purpose: Report per-indicator computation with timeframe
â”‚   â”‚   Example: "Processing AAPL (2/5) [1h] - Computing RSI (15/40)"
â”‚   â”‚
â”‚   â”œâ”€ on_fuzzy_generation(symbol, symbol_index, total_symbols,
â”‚   â”‚                       timeframe, fuzzy_set_name,
â”‚   â”‚                       fuzzy_index, total_fuzzy_sets)
â”‚   â”‚   Purpose: Report per-fuzzy-set generation with timeframe
â”‚   â”‚   Example: "Processing AAPL (2/5) [1h] - Fuzzifying macd_standard (12/40)"
â”‚   â”‚
â”‚   â””â”€ on_preparation_phase(phase, message)
â”‚       Purpose: Report data combining/splitting/model creation
â”‚       Example: "Combining data from 5 symbols"
â”‚
â””â”€ INTERNAL METHODS (unchanged)
    â”œâ”€ _emit(current_step, percentage, message, items_processed, phase, context)
    â”œâ”€ _check_cancelled()
    â”œâ”€ _derive_percentage()
    â””â”€ _clamp_items_processed()
```

**Key Design Points**:
- All new methods follow same pattern as existing methods
- Calculate simple percentages (pre-training: 0-5%, training: 5-95%)
- Build context-rich payload for renderer
- Use existing `_emit()` infrastructure

### 2. Training Orchestrators (Both Local and Host Service)

**Locations**:
- `ktrdr/api/services/training/local_orchestrator.py`
- `training-host-service/orchestrator.py` (in training host service)

**Current State**:
- `_create_progress_callback()` routes progress to bridge
- Currently handles: epoch progress, batch progress
- Both orchestrators have similar callback logic

**Changes Required**:

```
Both Orchestrators._create_progress_callback()
â”‚
â”œâ”€ EXISTING ROUTING (unchanged)
â”‚   â”œâ”€ progress_type == "batch" â†’ bridge.on_batch()
â”‚   â””â”€ default â†’ bridge.on_epoch()
â”‚
â””â”€ NEW ROUTING
    â”œâ”€ progress_type == "indicator_computation"
    â”‚   â†’ bridge.on_indicator_computation(...)
    â”‚
    â”œâ”€ progress_type == "fuzzy_generation"
    â”‚   â†’ bridge.on_fuzzy_generation(...)
    â”‚
    â”œâ”€ progress_type == "preprocessing"
    â”‚   â†’ bridge.on_symbol_processing(...)
    â”‚
    â””â”€ progress_type == "preparation"
        â†’ bridge.on_preparation_phase(...)
```

**Implementation Note**:
- Both orchestrators need **identical routing logic**
- Minor change since TrainingPipeline does the real work
- Just extracting fields from metrics dict and calling bridge methods

### 3. TrainingPipeline (Instrumented)

**Location**: `ktrdr/training/training_pipeline.py`

**Current State**:
- `train_strategy()` orchestrates entire training flow
- Has `progress_callback` parameter (already exists!)
- Currently only passes callback to `train_model()`

**Changes Required**:

```
TrainingPipeline.train_strategy()
â”‚
â”œâ”€ PHASE: Load market data
â”‚   â””â”€ ADD: Progress callbacks per timeframe load
â”‚       Callback: progress_callback(0, 0, {
â”‚           "progress_type": "preprocessing",
â”‚           "symbol": symbol,
â”‚           "symbol_index": idx,
â”‚           "total_symbols": len(symbols),
â”‚           "step": "loading_data",
â”‚           "timeframe": tf
â”‚       })
â”‚
â”œâ”€ PHASE: Calculate indicators
â”‚   â””â”€ MODIFY: calculate_indicators() to accept progress callback
â”‚       â””â”€ ADD: Progress callbacks per indicator per timeframe
â”‚           Callback: progress_callback(0, 0, {
â”‚               "progress_type": "indicator_computation",
â”‚               "symbol": symbol,
â”‚               "timeframe": tf,
â”‚               "indicator_name": name,
â”‚               "indicator_index": idx,
â”‚               "total_indicators": count
â”‚           })
â”‚
â”œâ”€ PHASE: Generate fuzzy memberships
â”‚   â””â”€ MODIFY: generate_fuzzy_memberships() to accept progress callback
â”‚       â””â”€ ADD: Progress callbacks per fuzzy set per timeframe
â”‚           Callback: progress_callback(0, 0, {
â”‚               "progress_type": "fuzzy_generation",
â”‚               "symbol": symbol,
â”‚               "timeframe": tf,
â”‚               "fuzzy_set_name": name,
â”‚               "fuzzy_index": idx,
â”‚               "total_fuzzy_sets": count
â”‚           })
â”‚
â”œâ”€ PHASE: Create features
â”‚   â””â”€ ADD: Single progress callback per symbol
â”‚       Callback: progress_callback(0, 0, {
â”‚           "progress_type": "preprocessing",
â”‚           "step": "creating_features"
â”‚       })
â”‚
â”œâ”€ PHASE: Generate labels
â”‚   â””â”€ ADD: Single progress callback per symbol
â”‚       Callback: progress_callback(0, 0, {
â”‚           "progress_type": "preprocessing",
â”‚           "step": "generating_labels"
â”‚       })
â”‚
â”œâ”€ PHASE: Combine data
â”‚   â””â”€ ADD: Single progress callback
â”‚       Callback: progress_callback(0, 0, {
â”‚           "progress_type": "preparation",
â”‚           "phase": "combining_data",
â”‚           "total_symbols": len(symbols)
â”‚       })
â”‚
â”œâ”€ PHASE: Split data
â”‚   â””â”€ ADD: Single progress callback
â”‚       Callback: progress_callback(0, 0, {
â”‚           "progress_type": "preparation",
â”‚           "phase": "splitting_data",
â”‚           "total_samples": len(data)
â”‚       })
â”‚
â”œâ”€ PHASE: Create model
â”‚   â””â”€ ADD: Single progress callback
â”‚       Callback: progress_callback(0, 0, {
â”‚           "progress_type": "preparation",
â”‚           "phase": "creating_model",
â”‚           "input_dim": dim
â”‚       })
â”‚
â”œâ”€ PHASE: Train model
â”‚   â””â”€ EXISTING: progress_callback passed through (UNCHANGED)
â”‚
â””â”€ PHASE: Evaluate model
    â””â”€ (Fast, no progress needed)
```

**Key Points**:
- Most changes are just inserting `if progress_callback:` checks
- Callback signature stays same: `callback(epoch, total_epochs, metrics)`
- For pre-training: pass `0, 0` for epoch/total_epochs, put data in metrics dict
- For training: existing behavior continues unchanged

### 4. IndicatorEngine and FuzzyEngine (UNCHANGED)

**Locations**:
- `ktrdr/indicators/indicator_engine.py`
- `ktrdr/fuzzy/fuzzy_engine.py`

**Current State**:
- Batch-compute all indicators/fuzzy sets
- No progress reporting

**Changes Required**: **NONE**

**Critical Design Decision**:
- IndicatorEngine and FuzzyEngine remain PURE COMPUTATION
- They know NOTHING about progress reporting
- Progress reporting happens in TrainingPipeline BEFORE calling these engines
- This keeps concerns separated: engines compute, pipeline orchestrates and reports

**Why this is better**:
- No changes to indicator/fuzzy computation logic
- No new parameters to thread through
- Engines remain testable in isolation
- Progress reporting is TrainingPipeline's responsibility (where it belongs)
- If engines are used elsewhere (non-training), they don't drag progress dependencies

### 5. TrainingProgressRenderer (Enhanced)

**Location**: `ktrdr/api/services/training/training_progress_renderer.py`

**Current State**:
- Renders epoch/batch messages: "Epoch 5/100 Â· Batch 120/500 ğŸ–¥ï¸ GPU: 85%"

**Changes Required**:

```
TrainingProgressRenderer.render_message(state)
â”‚
â”œâ”€ Check state.context.get("preprocessing_step")
â”‚
â”œâ”€ IF preprocessing_step == "computing_indicator":
â”‚   â””â”€ Format: "Processing {symbol} ({idx}/{total}) [{tf}] - Computing {indicator} ({idx}/{total})"
â”‚
â”œâ”€ ELIF preprocessing_step == "generating_fuzzy":
â”‚   â””â”€ Format: "Processing {symbol} ({idx}/{total}) [{tf}] - Fuzzifying {fuzzy} ({idx}/{total})"
â”‚
â”œâ”€ ELIF phase == "preprocessing":
â”‚   â””â”€ Format: "Processing {symbol} ({idx}/{total}) - {step}"
â”‚
â”œâ”€ ELIF phase == "preparation":
â”‚   â””â”€ Return: state.message (already formatted in bridge)
â”‚
â””â”€ ELSE (default):
    â””â”€ EXISTING LOGIC: Format epoch/batch (UNCHANGED)
```

**Key Points**:
- Enhanced, not replaced
- Checks context to determine message type
- Falls back to existing epoch/batch rendering
- All existing functionality preserved

---

## Data Structures

### Progress Callback Context Dictionary

The `progress_callback` is invoked with `(epoch, total_epochs, metrics)` where `metrics` is a dict:

**For indicator computation**:
```python
{
    "progress_type": "indicator_computation",
    "symbol": "AAPL",
    "symbol_index": 2,
    "total_symbols": 5,
    "timeframe": "1h",
    "indicator_name": "RSI",
    "indicator_index": 15,
    "total_indicators": 40
}
```

**For fuzzy generation**:
```python
{
    "progress_type": "fuzzy_generation",
    "symbol": "AAPL",
    "symbol_index": 2,
    "total_symbols": 5,
    "timeframe": "1h",
    "fuzzy_set_name": "rsi_14",
    "fuzzy_index": 10,
    "total_fuzzy_sets": 40
}
```

**For general preprocessing**:
```python
{
    "progress_type": "preprocessing",
    "symbol": "AAPL",
    "symbol_index": 2,
    "total_symbols": 5,
    "step": "loading_data" | "creating_features" | "generating_labels",
    "timeframe": "1h"  # optional
}
```

**For preparation phase**:
```python
{
    "progress_type": "preparation",
    "phase": "combining_data" | "splitting_data" | "creating_model",
    "total_symbols": 5,  # for combining
    "total_samples": 15847,  # for splitting
    "input_dim": 256  # for model creation
}
```

---

## Error Handling

### Progress During Failures

When errors occur mid-operation:

1. **Progress callback already invoked** showing current step
2. **Error context preserved** in OperationsService
3. **User sees** exactly where failure occurred

Example: If loading AAPL (symbol 2/5) fails, user sees:
```
Processing AAPL (2/5) [1h] - Loading data
ERROR: Failed to load AAPL: Connection timeout
```

No special error handling needed in progress system - errors propagate normally, last progress update shows context.

### Cancellation

Cancellation tokens already exist and work:
- TrainingPipeline checks `cancellation_token.is_cancelled()` periodically
- Progress reporting doesn't interfere with cancellation
- On cancellation, last progress update shows where it was cancelled

---

## Performance Considerations

### Overhead Analysis

**Per progress update**:
- Context dict creation: ~0.01ms
- Callback invocation: ~0.05ms
- Bridge formatting: ~0.1ms
- GenericProgressManager update: ~0.1ms
- Database write (async): ~1-5ms

**Total**: ~1-5ms per update (non-blocking)

**Updates per training**:
- Symbol-level: ~10-20 updates
- Indicator-level: ~200-400 updates (5 symbols Ã— 40 indicators Ã— 2 timeframes)
- Fuzzy-level: ~200-400 updates

**Total overhead**: ~500-1000ms over 3-5 minute training = **< 0.5%**

### Optimization Opportunities

If overhead becomes an issue (unlikely):
1. **Throttle updates** in tight loops (e.g., max 5 updates/second)
2. **Batch context updates** (reuse dicts)
3. **Lazy formatting** (only format when rendered)
4. **Skip updates** for very fast operations (< 100ms)

---

## Testing Strategy

### Unit Tests

**TrainingProgressBridge**:
- Test each new method formats correct message
- Test percentage calculations
- Test context propagation
- Test cancellation handling

**Orchestrators**:
- Test routing logic for new progress types
- Test context extraction from metrics dict
- Test both Local and Host Service orchestrators

**TrainingProgressRenderer**:
- Test preprocessing message formatting
- Test fallback to existing epoch/batch rendering
- Test context-based formatting

### Integration Tests

**TrainingPipeline**:
- Test progress callbacks invoked at correct points
- Test context includes correct symbol/timeframe/indicator info
- Test with actual strategy configuration

**End-to-End**:
- Test full training flow with progress reporting
- Verify progress updates appear in OperationsService
- Verify messages render correctly

---

## Summary

This architecture provides comprehensive progress reporting by:

1. **Extending existing bridge** with 4 new methods for pre-training progress
2. **Instrumenting TrainingPipeline** with progress callbacks at ~15 checkpoints
3. **Routing in both orchestrators** to handle new progress types
4. **Enhancing renderer** to format preprocessing messages
5. **ZERO changes** to IndicatorEngine/FuzzyEngine (they stay pure computation)

**Key Benefits**:
- Leverages existing infrastructure (no new systems)
- Minimal code changes (mostly adding callbacks)
- Zero silence during training (constant feedback)
- Granular visibility (per-indicator, per-fuzzy-set with timeframe)
- < 0.5% performance overhead

---

**Next**: Implementation plan breaking this into phased tasks.

---

**END OF ARCHITECTURE DOCUMENT**
