# Implementation Plan V2: Data Architecture Separation (Extraction-Focused)

## Document Information

- **Date**: 2025-01-29
- **Status**: PROPOSED - REVISED
- **Version**: 2.0 (Extraction-Focused Rewrite)
- **Supersedes**: Version 1.0 (Creation-Focused)
- **Starting Point**: Commit 79f98d6 (End of Phase 1)
- **Related Documents**:
  - [Design](./01-design-data-separation.md) - WHY (solution approach)
  - [Architecture](./02-architecture-data-separation.md) - WHAT (structure)
  - **This Document** - DETAILED WHAT + WHEN + ACCEPTANCE CRITERIA (REVISED)

---

## Executive Summary

### Why This Revision?

**Original Plan Problem**: Tasks tried to "create" components from scratch, forgetting battle-tested logic already exists in DataManager (~1500 LOC of working code).

**Result**: Task 2.2 became too big, LLM created `DEFERRED_WORK.md` as a red flag.

**Solution**: **EXTRACT** existing working code incrementally, not rewrite it.

### Key Changes from V1

| Aspect | V1 (Original) | V2 (Revised) |
|--------|--------------|--------------|
| **Approach** | "Create" components | "Extract" from DataManager |
| **Phase 2** | 12 tasks, 2-3 weeks | 6 tasks, 1 week |
| **Task Size** | Task 2.2 massive | All tasks atomic |
| **Risk** | High (rewriting) | Low (moving code) |
| **Deferred Work** | Required | Eliminated |

### New Phase Structure

| Phase | Duration | Deliverable | Tasks |
|-------|----------|-------------|-------|
| 0 | Done âœ… | Test baseline | 8 |
| 1 | Done âœ… | Host service ready | 5 |
| **2** | **1 week** | **Repository extracted** | **6** |
| **3** | **4-5 days** | **IB deleted, HTTP-only** | **7** |
| **4** | **1-2 weeks** | **Acquisition extracted** | **7** |
| **5** | **3-4 days** | **Clean architecture** | **6** |

**Total**: ~3-4 weeks (similar timeline, WAY less risk)

---

## Critical Principles for This Revision

### 1. Extract, Don't Create

```python
# âŒ WRONG (V1 approach):
class DataRepository:
    def load_from_cache(self):
        # Write from scratch
        # ... might forget edge cases

# âœ… RIGHT (V2 approach):
# FROM: LocalDataLoader.load() (existing, working)
# TO: DataRepository.load_from_cache() (extracted)
# Copy + refactor existing code
```

### 2. Each Task MUST Deliver Value

- Working end-to-end
- Passes integration tests
- All unit tests + quality checks pass
- Can ship to production if needed

### 3. No Giant Tasks

- If a task feels too big â†’ split it
- Maximum ~2 days per task
- Prefer 0.5-1 day tasks

---

## Starting Point: Commit 79f98d6 (End of Phase 1)

### What Exists

**File Structure**:
```
ktrdr/data/
â”œâ”€â”€ data_manager.py             # ALL logic here (~1500 LOC)
â”œâ”€â”€ data_manager_builder.py     # Builder pattern
â”œâ”€â”€ local_data_loader.py        # Local cache I/O
â”œâ”€â”€ ib_data_adapter.py          # IB integration (imports ktrdr.ib)
â”œâ”€â”€ data_loading_orchestrator.py
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ gap_classifier.py
â”‚   â”œâ”€â”€ gap_analyzer.py         # Gap analysis logic
â”‚   â”œâ”€â”€ segment_manager.py      # Segmentation logic
â”‚   â”œâ”€â”€ data_health_checker.py
â”‚   â”œâ”€â”€ data_processor.py
â”‚   â””â”€â”€ ... (other components)
â””â”€â”€ (NO repository/ directory)
â””â”€â”€ (NO acquisition/ directory)
```

**Key Points**:
- âœ… Phase 1 complete (host service ready)
- âŒ NO DataRepository yet
- âŒ NO DataAcquisitionService yet
- âŒ NO repository/ or acquisition/ directories
- âœ… DataManager has ALL working logic
- âœ… IbDataAdapter imports from `ktrdr.ib` (will be fixed Phase 3)

---

## Testing Guide

### Where Integration Tests Are Defined

**Integration test scenarios are fully documented in:**

1. **`docs/testing/SCENARIOS.md`** - Complete test scenario definitions
   - 13 data scenarios (D1.1-D4.3)
   - Training scenarios (1.1-4.2)
   - Step-by-step commands for each scenario
   - Expected results and performance baselines

2. **`docs/testing/TESTING_GUIDE.md`** - Building blocks for testing
   - API endpoint reference
   - Service URLs and ports
   - Common commands and scripts
   - Log access patterns

### How to Run Integration Tests

**For each task, run the relevant scenarios:**

```bash
# Example: Task 2.4 (Wire Repository to API)
# Run scenarios: D1.1, D1.2, D1.4

# D1.1: Load cached data
curl -s "http://localhost:8000/api/v1/data/EURUSD/1h" | jq '.data.dates | length'

# D1.2: Query data range
curl -s -X POST http://localhost:8000/api/v1/data/range \
  -H "Content-Type: application/json" \
  -d '{"symbol":"EURUSD","timeframe":"1h"}' | jq

# D1.4: List available data
curl -s "http://localhost:8000/api/v1/data/info" | jq '.data.total_symbols'
```

**Prerequisites for Integration Tests:**
- Backend running (`docker-compose up` or local API server)
- For IB tests: IB Host Service + IB Gateway running
- Sample data cached (EURUSD 1h recommended)

### Test Validation Requirements

Each task MUST pass:
1. âœ… All relevant integration test scenarios (see task acceptance criteria)
2. âœ… All unit tests (`make test-unit`)
3. âœ… Quality checks (`make quality`)

**Integration test scenarios are NOT optional** - they verify end-to-end functionality.

---

## Table of Contents

1. [Phase 0: Baseline Testing](#phase-0-baseline-testing) âœ…
2. [Phase 1: IB Code Duplication](#phase-1-ib-code-duplication-host-service-preparation) âœ…
3. [Phase 2: Repository Extraction](#phase-2-repository-extraction-cache-operations)
4. [Phase 3: IB Isolation](#phase-3-ib-isolation-http-only-provider)
5. [Phase 4: Acquisition Extraction](#phase-4-acquisition-extraction-download-logic)
6. [Phase 5: Cleanup & Documentation](#phase-5-cleanup--documentation)

---

## Phase 0: Baseline Testing âœ…

**Status**: COMPLETE

**What Was Done**:
- âœ… Created 13 integration test scenarios (D1.1-D4.3)
- âœ… Documented in `docs/testing/SCENARIOS.md`
- âœ… Validated all scenarios manually
- âœ… Validated with integration-test-specialist agent
- âœ… Established performance baselines

**Exit Criteria**: All met âœ…

---

## Phase 1: IB Code Duplication (Host Service Preparation) âœ…

**Status**: COMPLETE (with expected caveats)

**What Was Done**:
- âœ… Copied IB code to `ib-host-service/ib/` (8 files)
- âœ… Host service imports from local `ib/` package
- âœ… Host service endpoints work
- âœ… Integration tests D2.1-D2.3 pass
- âœ… Bug fixed: IB duration format Error 321

**What Was NOT Done** (expected, not a failure):
- âŒ Backend still imports from `ktrdr/ib` (IbDataAdapter needs it)
- âŒ Original `ktrdr/ib/` not deleted (Phase 3 will handle)

**Exit Status**: Host service self-contained and working âœ…

---

## Phase 2: Repository Extraction (Cache Operations)

### Goal

Extract cache operations from DataManager into standalone DataRepository.

### Context

**Current State** (at commit 79f98d6):
- âŒ NO DataRepository
- âŒ NO repository/ directory
- âœ… LocalDataLoader has basic I/O (save/load CSV)
- âœ… DataManager wraps LocalDataLoader with validation
- âœ… All cache logic working in DataManager

**Code to Extract From**:
- `LocalDataLoader` (lines 1-400) - Basic I/O
- `DataManager.load_data()` (lines 267-330) - with mode="local"
- `DataManager.get_data_summary()` (lines 1219-1266)
- `DataManager.merge_data()` (lines 1295-1335)

**Why This Phase**:
- Repository is simplest component (sync, no IB, no Operations)
- Can deliver value immediately (faster cache operations)
- No chicken-and-egg with IB code

**Strategy**:
1. Create repository/ directory structure
2. Extract DataQualityValidator first (used by Repository)
3. Create DataRepository by wrapping LocalDataLoader
4. Wire to API/CLI
5. Keep DataManager for downloads (Phase 4 will extract that)

### Duration

**1 week** (6 tasks)

### Tasks

---

#### TASK 2.1: Extract DataQualityValidator

**Objective**: Move DataQualityValidator from data/ to repository/.

**What to EXTRACT**:

Looking at current code, we need to create the validator first since Repository needs it.

**Files to CREATE**:
```
ktrdr/data/repository/
â”œâ”€â”€ __init__.py
â””â”€â”€ data_quality_validator.py
```

**Extraction Pattern**:

If DataQualityValidator already exists in `data/`:
1. Move it: `git mv data/data_quality_validator.py repository/data_quality_validator.py`
2. Update imports in DataManager

If it doesn't exist:
1. Extract validation logic from DataManager
2. Create new DataQualityValidator class
3. Use in DataManager (backward compatible)

**Files Modified**:
- Create: `ktrdr/data/repository/__init__.py`
- Create: `ktrdr/data/repository/data_quality_validator.py`
- Update: `ktrdr/data/data_manager.py` (import from new location)

**Scope**:
1. Create repository/ directory
2. Extract/create DataQualityValidator
3. Update DataManager imports
4. Add unit tests for validator

**Integration Test**: Validation still works in DataManager

**Acceptance Criteria**:
- [ ] `repository/` directory created
- [ ] `DataQualityValidator` exists in `repository/`
- [ ] DataManager uses validator from new location
- [ ] All validation tests pass
- [ ] All unit tests pass
- [ ] `make quality` passes

**Estimated Duration**: 0.5 days

---

#### TASK 2.2: Create DataRepository (Core Methods)

**Objective**: Create DataRepository by extracting cache I/O from DataManager.

**What to EXTRACT**:

```python
# FROM: DataManager.load_data() with mode="local" (lines 267-330)
# Extract local cache loading

# FROM: LocalDataLoader.load() (existing)
# Wrap with Repository

# TO: DataRepository.load_from_cache()
```

**Extraction Pattern**:

1. **Create Repository class**:
   ```python
   # NEW FILE: ktrdr/data/repository/data_repository.py

   from ktrdr.data.local_data_loader import LocalDataLoader
   from ktrdr.data.repository.data_quality_validator import DataQualityValidator

   class DataRepository:
       """Local cache repository for market data."""

       def __init__(self, data_dir: Optional[str] = None):
           self.loader = LocalDataLoader(data_dir=data_dir)
           self.validator = DataQualityValidator(auto_correct=False)

       def load_from_cache(self, symbol, timeframe, start_date=None, end_date=None):
           # Delegate to LocalDataLoader
           df = self.loader.load(symbol, timeframe, start_date, end_date)
           if df is None or df.empty:
               raise DataNotFoundError(...)
           return df

       def save_to_cache(self, symbol, timeframe, data):
           # Validate before save
           self.loader._validate_dataframe(data)
           # Delegate to LocalDataLoader
           self.loader.save(data, symbol, timeframe)
   ```

2. **Extract get_data_range**:
   ```python
   # FROM: DataManager.get_data_summary() (lines 1219-1266)
   # Extract date range logic

   def get_data_range(self, symbol, timeframe):
       date_range = self.loader.get_data_date_range(symbol, timeframe)
       if not date_range:
           raise DataNotFoundError(...)
       return {
           "symbol": symbol,
           "timeframe": timeframe,
           "start_date": date_range[0],
           "end_date": date_range[1],
       }
   ```

**Files Created**:
- `ktrdr/data/repository/data_repository.py`

**Files Modified**:
- `ktrdr/data/repository/__init__.py` (export DataRepository)

**Scope**:
1. Create DataRepository class
2. Implement `load_from_cache()`
3. Implement `save_to_cache()`
4. Implement `get_data_range()`
5. Add unit tests (mock LocalDataLoader)

**Integration Test**:
- Create temp file, save data, load data, verify

**Acceptance Criteria**:
- [ ] `DataRepository` created
- [ ] `load_from_cache()` works
- [ ] `save_to_cache()` works
- [ ] `get_data_range()` works
- [ ] Unit tests pass (25+ tests)
- [ ] Integration test passes (real file I/O)
- [ ] `make quality` passes

**Estimated Duration**: 1 day

---

#### TASK 2.3: Add Repository Helper Methods

**Objective**: Add convenience methods to DataRepository.

**What to EXTRACT**:

```python
# FROM: DataManager.get_data_summary() (lines 1219-1266)
# Extract summary logic

# FROM: LocalDataLoader methods
# Wrap with Repository interface
```

**Methods to Add**:

1. **get_available_symbols()** - List cached symbols
2. **delete_from_cache()** - Delete cached data
3. **get_cache_stats()** - Cache size, file count
4. **validate_data()** - Validate cached data

**Files Modified**:
- `ktrdr/data/repository/data_repository.py`

**Scope**:
1. Add helper methods
2. Extract logic from DataManager/LocalDataLoader
3. Add unit tests

**Integration Test**: Helper methods work correctly

**Acceptance Criteria**:
- [ ] Helper methods implemented
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] `make quality` passes

**Estimated Duration**: 0.5 days

---

#### TASK 2.4: Wire Repository to API (Read-Only)

**Objective**: Update GET endpoints to use DataRepository.

**What to REFACTOR**:

```python
# FROM: ktrdr/api/endpoints/data.py
# Current:
from ktrdr.data.data_manager import DataManager
data_manager = DataManager()

@router.get("/data/{symbol}/{timeframe}")
async def get_data(...):
    df = data_manager.load_data(symbol, timeframe, mode="local")

# TO:
from ktrdr.data.repository import DataRepository
data_repository = DataRepository()

@router.get("/data/{symbol}/{timeframe}")
async def get_data(...):
    df = data_repository.load_from_cache(symbol, timeframe)
```

**Files Modified**:
- `ktrdr/api/endpoints/data.py`

**Scope**:
1. Add DataRepository import
2. Update `GET /data/{symbol}/{timeframe}`
3. Update `GET /data/range`
4. Update `GET /data/info`
5. Keep `POST /data/load` using DataManager (unchanged)

**Integration Test**: D1.1, D1.2, D1.4 via API

**Acceptance Criteria**:
- [x] GET endpoints use DataRepository
- [x] POST endpoints still use DataManager
- [x] Integration tests D1.1, D1.2, D1.4 pass (see `docs/testing/SCENARIOS.md` for details)
- [x] All unit tests pass (1776/1776)
- [x] `make quality` passes

**Test Documentation**: See [Testing Guide](#testing-guide) section above for integration test locations and commands.

**Estimated Duration**: 1 day

**Actual Duration**: 1 day âœ…

**Status**: âœ… COMPLETE (Commit: 42aadb0)

---

#### TASK 2.5: Wire Repository to CLI (Read-Only)

**Status**: âŒ **NOT NEEDED** - Task based on incorrect assumption

**Why Not Needed**:

The implementation plan was written with an incorrect assumption about CLI architecture. Upon investigation, the CLI **already uses the API exclusively** and does NOT directly import DataManager or DataRepository:

```python
# ACTUAL IMPLEMENTATION (ktrdr/cli/data_commands.py)

# ktrdr data show (line 147-149)
async with AsyncCLIClient() as cli:
    data = await cli._make_request("GET", f"/data/{symbol}/{timeframe}", params=params)

# ktrdr data load (line 439)
response = await api_client.load_data(...)

# ktrdr data range (line 891)
data = await api_client.get_data_range(symbol=symbol, timeframe=timeframe)
```

**Architecture Verification**:

The CLI follows the correct architecture from CLAUDE.md:
```
CLI â†’ AsyncCLIClient â†’ API Endpoints â†’ Backend Services (DataRepository/DataManager)
```

**Impact**:
- âœ… **Task 2.4 complete**: API endpoints use DataRepository
- âœ… **CLI already correct**: Uses API, not direct imports
- âœ… **Zero changes needed**: API interface unchanged
- âœ… **No breaking changes**: Everything works without CLI modifications

**Conclusion**: Since the API endpoints were updated in Task 2.4 to use DataRepository, and the CLI already uses the API (not direct imports), there are no CLI changes required. The CLI automatically benefits from the API improvements.

**Files Verified**:
- `ktrdr/cli/data_commands.py` - Confirmed uses AsyncCLIClient/API only

**Original Estimated Duration**: 0.5 days â†’ **Actual Duration**: 0 days (not applicable)

---

#### TASK 2.6: Add Optional Repository to DataManager

**Objective**: Allow DataManager to optionally save via Repository.

**What to REFACTOR**:

```python
# FROM: DataManager.merge_data() (lines 1295-1335)
# Current:
def merge_data(self, ...):
    # ... merge logic
    if save_result:
        self.data_loader.save(merged_data, symbol, timeframe)

# TO:
def merge_data(self, ...):
    # ... merge logic
    if save_result:
        if hasattr(self, 'repository') and self.repository:
            self.repository.save_to_cache(symbol, timeframe, merged_data)
        else:
            self.data_loader.save(merged_data, symbol, timeframe)
```

**Files Modified**:
- `ktrdr/data/data_manager.py`

**Scope**:
1. Add optional `repository` parameter to `__init__`
2. Update save paths to use Repository if available
3. Maintain backward compatibility

**Integration Test**: D3.3 with Repository

**Acceptance Criteria**:
- [ ] DataManager accepts optional Repository
- [ ] Uses Repository for saves if provided
- [ ] Backward compatible without Repository
- [ ] All tests pass
- [ ] `make quality` passes

**Estimated Duration**: 0.5 days

---

### Phase 2 Exit Criteria

**Status**: âœ… **COMPLETE** (2025-10-31)

**Components**:
- [x] DataRepository exists and tested (Tasks 2.1-2.3)
- [x] DataQualityValidator in repository/ (Task 2.1)
- [x] API GET endpoints use Repository (Task 2.4)
- [x] CLI read commands use Repository (Task 2.5 NOT NEEDED - CLI uses API)
- [ ] DataManager optionally uses Repository (Task 2.6 SKIPPED - backward compat not needed)

**Integration Tests**:
- [x] D1.1-D1.4 pass (cache via Repository) - 4/4 PASSED
  - D1.1: Load EURUSD 1h - 115,243 bars in 608ms âœ…
  - D1.2: Range query - 115,243 points in 214ms âœ…
  - D1.3: Data validation - 4,762 bars validated in 43ms âœ…
  - D1.4: Data info - 32 symbols indexed in 356ms âœ…
- [x] D3.1-D3.3 still pass (downloads via DataManager) - 2/3 PASSED
  - D3.1: âš ï¸ FAILED (IB Gateway timeout - infrastructure issue, not code)
  - D3.2: âœ… 2,794 bars in 24s
  - D3.3: âœ… Fast reload in 564ms

**Code Quality**:
- [x] All unit tests pass (1776/1776 - 100%)
- [x] `make quality` passes (ruff + black + mypy)
- [x] No regressions

**Total Duration**: 1 week (4-5 days) âœ…

**Actual Duration**: 4 days (Tasks 2.1-2.4 completed, 2.5 not needed, 2.6 skipped)

**Key Decisions**:
1. Task 2.5 marked NOT NEEDED - CLI already uses API exclusively
2. Task 2.6 skipped - Optional backward compatibility not required (DataManager will be removed in Phase 5)

**Ready for Phase 3**: Repository working, ready to isolate IB âœ…

---

## Phase 3: IB Isolation (HTTP-Only Provider)

### Goal

Backend never imports `ktrdr/ib`, only calls IB via HTTP.

### Context

**Current Blocker**:
- `IbDataAdapter` imports from `ktrdr.ib` (line 33):
  ```python
  from ktrdr.ib import IbDataFetcher, IbErrorClassifier, IbErrorType
  ```
- Backend uses `IbDataAdapter` for download operations
- Can't delete `ktrdr/ib/` until backend stops importing it

**Solution**:
1. Move acquisition files to acquisition/ directory
2. Create `IbDataProvider` (HTTP-only, no IB imports)
3. Replace `IbDataAdapter` with `IbDataProvider`
4. Delete `IbDataAdapter`
5. Verify no IB imports
6. Delete `ktrdr/ib/` directory

### Duration

**4-5 days** (7 tasks)

### Tasks

---

#### TASK 3.1: Move Files to acquisition/ Directory

**Objective**: Organize acquisition-related files into acquisition/ directory.

**What to MOVE**:

```bash
# Create directory
mkdir ktrdr/data/acquisition/

# Move files from components/
git mv ktrdr/data/components/gap_analyzer.py ktrdr/data/acquisition/
git mv ktrdr/data/components/gap_classifier.py ktrdr/data/acquisition/
git mv ktrdr/data/components/segment_manager.py ktrdr/data/acquisition/

# Move orchestrator
git mv ktrdr/data/data_loading_orchestrator.py ktrdr/data/acquisition/

# Move interface
git mv ktrdr/data/external_data_interface.py ktrdr/data/acquisition/
```

**Files Modified**:
- Create: `ktrdr/data/acquisition/__init__.py`
- Update all imports in DataManager, tests, etc.

**Scope**:
1. Create acquisition/ directory
2. Move 5 files
3. Update imports everywhere
4. Run tests

**Integration Test**: All tests still pass

**Acceptance Criteria**:
- [ ] acquisition/ directory created
- [ ] Files moved successfully
- [ ] Imports updated
- [ ] All tests pass
- [ ] `make quality` passes

**Estimated Duration**: 0.5 days

---

#### TASK 3.2: Create IbDataProvider (HTTP-Only)

**Objective**: Extract HTTP-only code from IbDataAdapter.

**What to EXTRACT**:

From `IbDataAdapter` (735 lines), keep only HTTP code, delete direct IB code.

See detailed extraction pattern in full plan above (Phase 3, Task 3.1 from previous version).

**Files Created**:
- `ktrdr/data/acquisition/ib_data_provider.py`

**Expected Size**: ~400-500 lines (vs 735)

**Acceptance Criteria**:
- [ ] `IbDataProvider` created
- [ ] HTTP-only (no IB imports)
- [ ] Unit tests pass
- [ ] Integration test D2.2 passes

**Estimated Duration**: 1.5 days

---

#### TASK 3.3: Update DataManager to Use IbDataProvider

**Objective**: Replace IbDataAdapter with IbDataProvider in DataManager.

**Files Modified**:
- `ktrdr/data/data_manager.py`

**Scope**: Update imports and initialization

**Acceptance Criteria**:
- [ ] DataManager uses IbDataProvider
- [ ] Integration tests D3.1-D3.3 pass

**Estimated Duration**: 0.5 days

---

#### TASK 3.4: Delete IbDataAdapter

**Objective**: Remove unused IbDataAdapter.

**Files Deleted**:
- `ktrdr/data/ib_data_adapter.py`

**Acceptance Criteria**:
- [ ] File deleted
- [ ] No imports found
- [ ] All tests pass

**Estimated Duration**: 0.5 days

---

#### TASK 3.5: Verify No IB Imports

**Objective**: Ensure backend never imports `ktrdr/ib`.

**Verification**:
```bash
grep -r "from ktrdr.ib import" ktrdr/ --exclude-dir=ib
# Should be ZERO results
```

**Acceptance Criteria**:
- [ ] No IB imports in backend
- [ ] Documentation created

**Estimated Duration**: 0.5 days

---

#### TASK 3.6: Delete ktrdr/ib/ Directory

**Objective**: Remove IB code from backend.

**Files Deleted**:
- `ktrdr/ib/` (entire directory, 9 files)

**Acceptance Criteria**:
- [ ] Directory deleted
- [ ] Docker rebuilds successfully
- [ ] Backend starts without errors
- [ ] All integration tests pass

**Estimated Duration**: 0.5 days

---

#### TASK 3.7: Final Integration Test

**Objective**: Validate all operations work after IB deletion.

**What to TEST**: Run ALL Phase 0 scenarios (D1.1-D4.3)

**Acceptance Criteria**:
- [x] ALL Phase 0 tests pass
- [x] No regressions
- [x] Performance acceptable

**Estimated Duration**: 0.5 days

---

### Phase 3 Exit Criteria

**Code Removed**:
- [x] IbDataAdapter deleted
- [x] ktrdr/ib/ deleted

**Components Created**:
- [x] IbDataProvider (HTTP-only)

**Import Rules**:
- [x] Backend has ZERO imports from ktrdr/ib
- [x] Backend calls IB only via HTTP

**Integration Tests**:
- [x] ALL Phase 0 tests pass

**Total Duration**: 4-5 days

**Ready for Phase 4**: Backend IB-free âœ…

---

## Phase 4: Acquisition Extraction (Download Logic)

### Goal

Extract download logic from DataManager into DataAcquisitionService.

### Context

**Current State**:
- âœ… DataRepository (Phase 2) - cache
- âœ… IbDataProvider (Phase 3) - IB HTTP
- âŒ NO DataAcquisitionService (Tasks 4.1-4.2 create shell)
- âœ… DataManager has ALL download logic

**Strategy**: Extract methods from DataManager incrementally.

### Critical Success Factors

**1. Backward Compatibility During Phase 4**:

**IMPORTANT - Code Deletion Strategy**:
- âŒ **DO NOT delete extracted code from DataManager during Phase 4**
- âœ… **Keep both implementations in parallel** for safety
- âœ… **New endpoints use DataAcquisitionService**
- âœ… **DataManager code deleted in Phase 5 ONLY**

**Phase 4 Coexistence**:
- DataManager remains 100% functional throughout Phase 4
- DataManager's extracted methods (head timestamp, gap analysis, etc.) **stay in DataManager**
- POST /data/load routes to **DataAcquisitionService** (not DataManager)
- Both services coexist for rollback safety
- If DataAcquisitionService has issues, can quickly revert to DataManager

**Phase 5 Deletion**:
- After Phase 4 validates DataAcquisitionService works correctly
- Delete DataManager entirely
- Delete all extracted methods from original location
- Remove backward compatibility shims

**2. Task Dependency Order**:

```
Task 4.1 â”€â”€> Task 4.2 â”€â”€> Task 4.3 â”€â”€> Task 4.4 â”€â”€> Task 4.7
                              â”‚                           â–²
                              â”‚                           â”‚
                              â””â”€â”€> Task 4.6 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      (Can run in parallel)

Task 4.5: Independent review (can run anytime)
```

**Dependencies**:
- **Task 4.3 MUST complete before 4.4** (provides gap analysis)
- **Task 4.6 can run in parallel with 4.3-4.5** (independent progress work)
- **Task 4.7 requires 4.3-4.6 complete** (wires everything together)
- **Task 4.5 is independent** (review/refactor orchestrator)

**3. Configuration Management**:

All configurable values use environment variable overrides:

```bash
# Environment variables for DataAcquisitionService
export DATA_MAX_SEGMENT_SIZE=5000        # Max bars per download segment
export DATA_PERIODIC_SAVE_MIN=0.5        # Minutes between incremental saves
```

**4. Error Handling Philosophy**:

- **Fail gracefully**: Head timestamp failures fall back to defaults
- **Partial success**: Failed segments don't stop entire download
- **Clear logging**: All errors logged with context
- **User-friendly**: Error messages explain what happened and next steps

### Duration

**1-2 weeks** (7 tasks)

### Tasks

---

#### TASK 4.1: Create DataAcquisitionService Shell

**Objective**: Create basic service structure.

**What to CREATE**:

```python
# NEW FILE: ktrdr/data/acquisition/acquisition_service.py

from ktrdr.async_infrastructure.service_orchestrator import ServiceOrchestrator

class DataAcquisitionService(ServiceOrchestrator):
    """External data acquisition orchestrator."""

    def __init__(self, repository=None, provider=None):
        super().__init__()
        self.repository = repository or DataRepository()
        self.provider = provider or IbDataProvider()
```

**Files Created**:
- `ktrdr/data/acquisition/acquisition_service.py`

**Acceptance Criteria**:
- [ ] Service created
- [ ] Inherits from ServiceOrchestrator
- [ ] Basic structure in place

**Estimated Duration**: 0.5 days

---

#### TASK 4.2: Extract Basic Download Flow

**Objective**: Extract cache-check â†’ download â†’ save from DataManager.

**What to EXTRACT**: See detailed extraction in full plan above.

**Acceptance Criteria**:
- [ ] Basic download flow works
- [ ] Integration test passes

**Estimated Duration**: 2 days

---

#### TASK 4.3: Integrate Gap Analysis and Mode-Based Logic

**Status**: ðŸ”´ **NOT STARTED**

**Objective**: Extract and integrate gap analysis logic from DataManager into DataAcquisitionService.

**What to EXTRACT**:

From DataManager, extract the gap analysis workflow:

1. **Gap Analysis** (`gap_analyzer` component):
   - Already instantiated as component in DataManager (`__init__` line 137)
   - Used by `DataLoadingOrchestrator.load_with_fallback()`
   - Key methods:
     - `analyze_gaps_by_mode(existing_data, mode, start_date, end_date)` - mode-aware gap detection
     - `detect_gaps(data, timeframe)` - find internal gaps in data
     - `prioritize_gaps_by_mode(gaps, mode)` - sort gaps by download mode priority

2. **Mode-Based Download Logic**:
   - **tail mode**: Download recent data (missing at end)
   - **backfill mode**: Download historical data (missing at beginning)
   - **full mode**: Download complete range (fill all gaps)

3. **Head Timestamp Validation**:
   - `_fetch_head_timestamp_async()` - get earliest available data from provider
   - `_validate_request_against_head_timestamp()` - validate requested range
   - `_ensure_symbol_has_head_timestamp()` - cache head timestamps per symbol

**Implementation Pattern**:

```python
# In DataAcquisitionService

from ktrdr.data.acquisition.gap_analyzer import GapAnalyzer
from ktrdr.data.components.symbol_cache import SymbolCache

class DataAcquisitionService(ServiceOrchestrator[IbDataProvider]):
    def __init__(self, ...):
        # ... existing code ...
        self.gap_analyzer = GapAnalyzer()
        self.symbol_cache = SymbolCache()  # For caching head timestamps

    async def download_data(
        self,
        symbol: str,
        timeframe: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        mode: str = "tail",  # tail, backfill, full (NEW PARAMETER)
        ...
    ):
        # 1. Check cache (existing)
        try:
            existing_data = self.repository.load_from_cache(symbol, timeframe)
        except DataNotFoundError:
            existing_data = None

        # 2. Validate against head timestamp (NEW)
        head_timestamp = await self._fetch_head_timestamp(symbol, timeframe)
        await self._validate_request_against_head_timestamp(
            symbol, timeframe, start_date, end_date, head_timestamp
        )

        # 3. Analyze gaps based on mode (NEW)
        gaps = self.gap_analyzer.analyze_gaps_by_mode(
            existing_data=existing_data,
            mode=mode,
            start_date=start_date or head_timestamp,
            end_date=end_date or datetime.now(),
            timeframe=timeframe,
        )

        # 4. Convert gaps to download segments (next task)
        # 5. Download segments (next task)
        # 6. Save to cache
```

**Files Created**:
- None (uses existing GapAnalyzer)

**Files Modified**:
- `ktrdr/data/acquisition/acquisition_service.py` - Add gap analysis integration

**Methods to Extract from DataManager**:

1. **`_fetch_head_timestamp_async()`** (lines 953-982):
   - Calls `provider.get_head_timestamp(symbol, timeframe)`
   - Caches result in symbol cache
   - Returns earliest available timestamp

2. **`_validate_request_against_head_timestamp()`** (lines 1001-1061):
   - Validates requested date range
   - Adjusts start_date if before head timestamp
   - Logs warnings for data availability issues

3. **`_ensure_symbol_has_head_timestamp()`** (lines 1093-1122):
   - Ensures head timestamp is cached for symbol
   - Fetches if not in cache

**Scope**:
0. **Add `mode` parameter** to `download_data()` signature (PREREQUISITE)
1. Add `gap_analyzer` to DataAcquisitionService
2. Add `symbol_cache` to DataAcquisitionService (for head timestamp caching)
3. Extract head timestamp methods
4. Update `download_data()` to use gap analysis
5. Add mode-based gap detection
6. Add error handling for head timestamp failures
7. Add unit tests for gap integration
8. Add integration tests for mode-based downloads

**Error Handling Strategy**:

```python
# Error handling for head timestamp fetch
try:
    head_timestamp = await self._fetch_head_timestamp(symbol, timeframe)
except Exception as e:
    logger.warning(f"Failed to fetch head timestamp for {symbol}: {e}")
    # Fall back to reasonable default (e.g., 10 years ago)
    head_timestamp = datetime.now() - timedelta(days=365*10)

# Error handling for gap analysis
try:
    gaps = self.gap_analyzer.analyze_gaps_by_mode(...)
except Exception as e:
    logger.error(f"Gap analysis failed for {symbol}: {e}")
    # Fall back to full download
    gaps = [(start_date or head_timestamp, end_date or datetime.now())]
```

**Unit Test Specifications** (15+ tests):

**Basic Integration Tests**:
- `test_gap_analyzer_initialization` - Verify GapAnalyzer instantiated correctly
- `test_symbol_cache_initialization` - Verify SymbolCache instantiated correctly
- `test_mode_parameter_added_to_signature` - Verify mode parameter exists with default

**Head Timestamp Tests**:
- `test_fetch_head_timestamp_caches_result` - First fetch calls provider, second uses cache
- `test_fetch_head_timestamp_handles_provider_failure` - Falls back gracefully on error
- `test_validate_request_adjusts_start_date_before_head` - Adjusts start_date if before head timestamp
- `test_validate_request_logs_warning_for_old_dates` - Warns when requested date very old
- `test_ensure_symbol_has_head_timestamp_fetches_if_missing` - Fetches when not in cache

**Gap Analysis Tests**:
- `test_mode_tail_detects_end_gaps` - Tail mode identifies missing recent data
- `test_mode_backfill_detects_start_gaps` - Backfill mode identifies missing historical data
- `test_mode_full_detects_all_gaps` - Full mode identifies all missing data ranges
- `test_gap_analysis_with_no_cache` - Handles case when no cached data exists
- `test_gap_analysis_with_complete_cache` - Returns empty gaps when cache complete

**Error Handling Tests**:
- `test_head_timestamp_failure_uses_fallback` - Uses default when head timestamp fails
- `test_gap_analysis_failure_falls_back_to_full` - Falls back to full download on gap error
- `test_invalid_mode_raises_error` - Validates mode parameter (tail/backfill/full only)

**Integration Test Scenarios**:
- D3.1: Download EURUSD 1h (tail mode) - should detect gap at end
- D3.2: Download AAPL 1d (backfill mode) - should detect gap at beginning
- D3.3: Download with full mode - should detect all gaps

**Acceptance Criteria**:
- [ ] GapAnalyzer integrated into DataAcquisitionService
- [ ] Head timestamp validation works
- [ ] Mode-based gap detection works (tail, backfill, full)
- [ ] Gaps correctly prioritized by mode
- [ ] Unit tests pass (15+ tests)
- [ ] Integration tests D3.1-D3.3 pass
- [ ] `make quality` passes

**Estimated Duration**: 2 days

**Branch Strategy**: Continue on `feature/data-architecture-separation`

---

#### TASK 4.4: Integrate Segment Manager for Resilient Downloads

**Status**: ðŸ”´ **NOT STARTED**

**Objective**: Integrate SegmentManager for resilient, progress-tracked segment downloads.

**What to EXTRACT**:

From DataManager's `_fetch_segments_with_component_async()` (lines 708-794):

1. **Segment Creation**:
   - Convert gaps into download segments
   - `SegmentManager.create_segments(gaps, max_segment_size)`
   - `SegmentManager.prioritize_segments(segments, mode)`

2. **Resilient Fetching**:
   - `SegmentManager.fetch_segments_with_resilience()` with:
     - Progress tracking via GenericProgressManager
     - Cancellation token support
     - Periodic save callback (every 0.5 minutes)
     - Retry logic with exponential backoff
     - Error recovery (continue on failed segments)

3. **Periodic Save Logic**:
   - `_save_periodic_progress()` from DataManager (lines 889-951)
   - Saves downloaded data incrementally during long operations
   - Merges with existing cache

**Implementation Pattern**:

```python
# In DataAcquisitionService

from ktrdr.data.acquisition.segment_manager import SegmentManager

class DataAcquisitionService(ServiceOrchestrator[IbDataProvider]):
    def __init__(self, ...):
        # ... existing code ...
        self.segment_manager = SegmentManager()

    async def download_data(self, ...):
        # ... gap analysis from Task 4.3 ...

        # 4. Create download segments (NEW)
        segments = self.segment_manager.create_segments(
            gaps=gaps,
            max_segment_size=5000,  # Max bars per segment
        )

        segments = self.segment_manager.prioritize_segments(
            segments=segments,
            mode=mode,
        )

        # 5. Download segments with resilience (NEW)
        # Note: progress_manager and cancellation_token come from ServiceOrchestrator
        # They are available within the operation_func passed to start_managed_operation()
        successful_data, successful_count, failed_count = \
            await self.segment_manager.fetch_segments_with_resilience(
                symbol=symbol,
                timeframe=timeframe,
                segments=segments,
                external_provider=self.provider,
                progress_manager=progress_manager,  # From ServiceOrchestrator context
                cancellation_token=cancellation_token,  # From ServiceOrchestrator context
                periodic_save_callback=self._create_periodic_save_callback(symbol, timeframe),
                periodic_save_minutes=self.periodic_save_interval,
            )

        # 6. Merge and save
        if successful_data:
            # Combine all downloaded segments
            combined = pd.concat(successful_data, ignore_index=False)
            combined = combined.sort_index()  # Ensure chronological order

            if existing_data is not None and not existing_data.empty:
                # Merge with existing cache using Repository's merge_data
                merged_data = self.repository.merge_data(existing_data, combined)
            else:
                merged_data = combined

            self.repository.save_to_cache(symbol, timeframe, merged_data)
```

**Configuration Constants**:

```python
# In DataAcquisitionService class definition
class DataAcquisitionService(ServiceOrchestrator[IbDataProvider]):
    # Configuration constants with environment variable overrides
    MAX_SEGMENT_SIZE = int(os.getenv("DATA_MAX_SEGMENT_SIZE", "5000"))
    PERIODIC_SAVE_INTERVAL = float(os.getenv("DATA_PERIODIC_SAVE_MIN", "0.5"))

    def __init__(self, ...):
        # Use class constants for configuration
        self.max_segment_size = self.MAX_SEGMENT_SIZE
        self.periodic_save_interval = self.PERIODIC_SAVE_INTERVAL
```

**Files Modified**:
- `ktrdr/data/acquisition/acquisition_service.py`

**Methods to Extract**:

1. **`_save_periodic_progress()`** (lines 889-951):
   - Merges downloaded data with existing cache
   - Saves incrementally during long downloads
   - Returns count of bars saved

2. **`_create_periodic_save_callback()`** (new wrapper):
   - Creates callback closure for segment manager
   - Captures symbol/timeframe context

**Scope**:
1. Add `segment_manager` to DataAcquisitionService
2. Extract periodic save logic
3. Integrate segment fetching into download flow
4. Add progress updates for segment downloads
5. Handle failed segments gracefully
6. Add unit tests for segment integration
7. Add integration tests for resilient downloads

**Error Handling Strategy**:

```python
# Handle segment creation failures
try:
    segments = self.segment_manager.create_segments(gaps, self.max_segment_size)
except Exception as e:
    logger.error(f"Segment creation failed: {e}")
    # Fall back to single segment covering entire gap
    segments = [(start_date, end_date)]

# Handle partial segment failures
if failed_count > 0:
    logger.warning(
        f"Download completed with {failed_count} failed segments. "
        f"Successfully downloaded {successful_count} segments."
    )
    # Don't raise error - partial success is acceptable
```

**Unit Test Specifications** (20+ tests):

**Segment Creation Tests**:
- `test_segment_manager_initialization` - Verify SegmentManager instantiated
- `test_create_segments_from_single_gap` - Single gap â†’ multiple segments
- `test_create_segments_respects_max_size` - Segments don't exceed MAX_SEGMENT_SIZE
- `test_prioritize_segments_tail_mode` - Tail mode prioritizes recent segments first
- `test_prioritize_segments_backfill_mode` - Backfill mode prioritizes old segments first
- `test_prioritize_segments_full_mode` - Full mode uses natural order

**Resilient Fetching Tests**:
- `test_fetch_segments_with_resilience_retries_failures` - Failed segments retried
- `test_fetch_segments_exponential_backoff` - Retry delay increases exponentially
- `test_fetch_segments_max_retries_reached` - Gives up after max retries
- `test_fetch_segments_partial_success_continues` - Continues after partial failures
- `test_fetch_segments_cancellation_stops_download` - Cancellation token works

**Periodic Save Tests**:
- `test_periodic_save_callback_created` - Callback closure captures context
- `test_periodic_save_merges_with_cache` - Incremental save merges correctly
- `test_periodic_save_interval_respected` - Saves happen at configured interval
- `test_periodic_save_handles_merge_failures` - Handles merge errors gracefully

**Data Merging Tests**:
- `test_merge_multiple_segments_chronologically` - Segments merged in order
- `test_merge_with_existing_data_removes_duplicates` - Duplicates removed
- `test_merge_with_no_existing_data` - Handles empty cache case
- `test_merge_respects_index_order` - Final data sorted chronologically

**Configuration Tests**:
- `test_max_segment_size_configurable` - MAX_SEGMENT_SIZE can be overridden
- `test_periodic_save_interval_configurable` - PERIODIC_SAVE_INTERVAL can be overridden

**Integration Test Scenarios**:
- D3.1: Download with progress tracking - verify incremental saves
- D3.2: Download with simulated failure - verify retry logic
- D3.3: Download with cancellation - verify partial save

**Acceptance Criteria**:
- [ ] SegmentManager integrated into DataAcquisitionService
- [ ] Segments created from gaps correctly
- [ ] Resilient fetching with retry works
- [ ] Periodic save during download works
- [ ] Failed segments don't stop entire download
- [ ] Progress tracking shows segment-level detail
- [ ] Unit tests pass (20+ tests)
- [ ] Integration tests D3.1-D3.3 pass
- [ ] `make quality` passes

**Estimated Duration**: 2-3 days

---

#### TASK 4.5: Integrate DataLoadingOrchestrator

**Status**: ðŸ”´ **NOT STARTED**

**Objective**: Refactor DataLoadingOrchestrator to work with Repository + Provider instead of DataManager.

**What to REFACTOR**:

The `DataLoadingOrchestrator` (lines 35-512 in `data_loading_orchestrator.py`) currently:

1. **Takes DataManager reference**:
   - `__init__(self, data_manager)` - stores reference
   - Calls `data_manager.load_data()` for cache access
   - Calls `data_manager.external_provider` for IB access

2. **Orchestrates full download flow**:
   - `load_with_fallback()` - main method
   - Cache check â†’ Gap analysis â†’ Segment fetching â†’ Merge â†’ Save
   - Handles all three modes: tail, backfill, full
   - Progress tracking integration
   - Fallback to local on IB failure

**Refactoring Pattern**:

```python
# BEFORE (current):
class DataLoadingOrchestrator:
    def __init__(self, data_manager):
        self.data_manager = data_manager

    def load_with_fallback(self, ...):
        # Cache check
        cached = self.data_manager.load_data(symbol, timeframe, mode="local")
        # Download
        self.data_manager.external_provider.fetch_historical_data(...)

# AFTER (refactored):
class DataLoadingOrchestrator:
    def __init__(self, repository, provider):
        self.repository = repository  # DataRepository for cache
        self.provider = provider      # IbDataProvider for downloads

    def load_with_fallback(self, ...):
        # Cache check
        cached = self.repository.load_from_cache(symbol, timeframe)
        # Download
        self.provider.fetch_historical_data(...)
```

**Implementation Strategy**:

Since Tasks 4.3-4.4 already integrate gap analysis and segment fetching directly into DataAcquisitionService, we have two options:

**Option A: Simplify Orchestrator** (RECOMMENDED)
- DataAcquisitionService directly orchestrates download flow
- Keep orchestrator as optional advanced fallback logic
- Remove DataManager dependency

**Option B: Use Orchestrator as Main Flow**
- DataAcquisitionService delegates to orchestrator
- Orchestrator takes Repository + Provider + GapAnalyzer + SegmentManager
- More complex but more modular

**Decision**: Use **Option A** - DataAcquisitionService is already the orchestrator (inherits from ServiceOrchestrator). The `DataLoadingOrchestrator` can be simplified or marked for deprecation in Phase 5.

**Scope**:
1. Review if orchestrator is still needed post Tasks 4.3-4.4
2. If needed, refactor to take Repository + Provider
3. Update DataAcquisitionService to use refactored orchestrator
4. If not needed, document for Phase 5 removal
5. Update tests

**Acceptance Criteria**:
- [ ] Orchestrator dependencies clarified
- [ ] Either refactored or marked for deprecation
- [ ] All tests pass
- [ ] `make quality` passes

**Estimated Duration**: 1 day

---

#### TASK 4.6: Add Enhanced Progress Tracking

**Status**: ðŸ”´ **NOT STARTED**

**Objective**: Add comprehensive progress tracking throughout the download flow.

**What to EXTRACT**:

From DataManager's progress tracking infrastructure:

1. **GenericProgressManager** (already in async_infrastructure):
   - Used in `_load_data_core_logic()` (lines 379-580)
   - `start_operation()`, `update_progress()`, `complete_operation()`
   - Integrates with DataProgressRenderer

2. **DataProgressRenderer** (in `data/async_infrastructure/`):
   - Data-specific progress display
   - Shows: symbol, timeframe, bars downloaded, ETA
   - Used by GenericProgressManager

3. **TimeEstimationEngine** (in `async_infrastructure/`):
   - Estimates completion time
   - Learns from historical operations

4. **Progress Context**:
   - Operation metadata (symbol, timeframe, mode, dates)
   - Current item detail (e.g., "Downloading segment 3/5")
   - Step descriptions

**Implementation Pattern**:

**Note**: ServiceOrchestrator's `start_managed_operation()` already creates a progress manager internally. This task focuses on **enhancing** the progress tracking with data-specific context and detailed step updates.

```python
# In DataAcquisitionService.download_data()

# Option 1: Let ServiceOrchestrator handle progress (RECOMMENDED)
# - start_managed_operation() creates progress manager automatically
# - We just need to update `total_steps` and add progress updates within operation_func

# Option 2: Manual progress manager (if needed for custom rendering)
from ktrdr.async_infrastructure.progress import GenericProgressManager
from ktrdr.data.async_infrastructure.data_progress_renderer import DataProgressRenderer

progress_manager = GenericProgressManager(
    callback=progress_callback,  # From parameter or ServiceOrchestrator
    renderer=DataProgressRenderer()
)

# Start operation
progress_manager.start_operation(
    operation_id=f"download_{symbol}_{timeframe}",
    total_steps=6,  # cache check, head timestamp, gap analysis, segment creation, download, save
    context={
        "symbol": symbol,
        "timeframe": timeframe,
        "mode": mode,
        "operation_type": "data_download",
    }
)

# Update throughout flow
progress_manager.update_progress(
    step=1,
    message="Checking cache for existing data",
    context={"current_item_detail": f"Loading {symbol} {timeframe} from cache"}
)

progress_manager.update_progress(
    step=2,
    message="Validating data range with provider",
    context={"current_item_detail": "Fetching head timestamp"}
)

# ... etc for each major step

# Complete
progress_manager.complete_operation()
```

**Progress Steps** (6 total):
1. Check cache for existing data
2. Fetch and validate head timestamp
3. Analyze gaps based on mode
4. Create download segments
5. Download segments (with sub-progress per segment)
6. Merge and save to cache

**Files Modified**:
- `ktrdr/data/acquisition/acquisition_service.py`

**Scope**:
1. Add DataProgressRenderer to DataAcquisitionService
2. Add GenericProgressManager integration
3. Add progress updates for all 6 steps
4. Add segment-level sub-progress
5. Add time estimation
6. Test progress callbacks work correctly

**Acceptance Criteria**:
- [ ] Progress tracking for all download steps
- [ ] Segment-level progress detail
- [ ] Time estimation works
- [ ] Progress callbacks fire correctly
- [ ] Progress display shows meaningful context
- [ ] Integration tests show progress updates
- [ ] `make quality` passes

**Estimated Duration**: 1-2 days

---

#### TASK 4.7: Wire DataAcquisitionService to API and CLI

**Status**: ðŸ”´ **NOT STARTED**

**Objective**: Create new API endpoints and update CLI to use DataAcquisitionService.

**What to CREATE**:

### API Changes

**1. New Endpoint: POST /data/acquire/download**

```python
# In ktrdr/api/endpoints/data.py

from ktrdr.api.dependencies import get_acquisition_service

@router.post(
    "/data/acquire/download",
    response_model=DataLoadOperationResponse,
    tags=["Data Acquisition"],
    summary="Download data from external provider",
    description="Download market data from external provider (IB) with gap analysis and progress tracking."
)
async def download_data(
    request: DataLoadRequest,
    acquisition_service: DataAcquisitionService = Depends(get_acquisition_service),
) -> DataLoadOperationResponse:
    """
    Download data from external provider (IB).

    This endpoint uses DataAcquisitionService for:
    - Intelligent gap analysis
    - Mode-based downloads (tail, backfill, full)
    - Progress tracking via Operations service
    - Resilient segment fetching

    Returns operation_id for tracking progress via GET /operations/{operation_id}
    """
    result = await acquisition_service.download_data(
        symbol=request.symbol,
        timeframe=request.timeframe,
        start_date=request.start_date,
        end_date=request.end_date,
        mode=request.mode or "tail",
    )

    return DataLoadOperationResponse(
        success=True,
        operation_id=result["operation_id"],
        status=result["status"],
        message=f"Started {request.mode} download for {request.symbol} {request.timeframe}",
    )
```

**2. Deprecate POST /data/load**

```python
@router.post(
    "/data/load",
    response_model=DataLoadOperationResponse,
    tags=["Data"],
    deprecated=True,  # Mark as deprecated
    summary="[DEPRECATED] Load data - use /data/acquire/download instead",
)
async def load_data_deprecated(
    request: DataLoadRequest,
    acquisition_service: DataAcquisitionService = Depends(get_acquisition_service),
) -> DataLoadOperationResponse:
    """
    DEPRECATED: Use POST /data/acquire/download instead.

    This endpoint routes to the new acquisition service but is deprecated.
    """
    logger.warning(
        "POST /data/load is deprecated. Use POST /data/acquire/download instead."
    )

    # Route to new endpoint
    result = await acquisition_service.download_data(
        symbol=request.symbol,
        timeframe=request.timeframe,
        start_date=request.start_date,
        end_date=request.end_date,
        mode=request.mode or "tail",
    )

    return DataLoadOperationResponse(
        success=True,
        operation_id=result["operation_id"],
        status=result["status"],
        message=f"[DEPRECATED] Use /data/acquire/download. Started download for {request.symbol}",
        deprecated=True,
    )
```

**3. Add Dependency Injection**

```python
# In ktrdr/api/dependencies.py

from ktrdr.data.acquisition import DataAcquisitionService

_acquisition_service: Optional[DataAcquisitionService] = None

def get_acquisition_service() -> DataAcquisitionService:
    """Get or create DataAcquisitionService singleton."""
    global _acquisition_service
    if _acquisition_service is None:
        _acquisition_service = DataAcquisitionService()
    return _acquisition_service
```

### CLI Changes

**Update `ktrdr data load` command**:

```python
# In ktrdr/cli/data_commands.py

@data_app.command("load")
def load_data(
    symbol: str,
    timeframe: str = "1d",
    mode: str = "tail",  # tail, backfill, full
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
):
    """
    Download data from external provider (IB).

    Uses new /data/acquire/download endpoint with DataAcquisitionService.

    Modes:
    - tail: Download recent data (fill gaps at end)
    - backfill: Download historical data (fill gaps at beginning)
    - full: Download complete range (fill all gaps)

    Examples:
        ktrdr data load AAPL --mode tail
        ktrdr data load EURUSD --timeframe 1h --mode backfill --start 2024-01-01
        ktrdr data load MSFT --mode full --start 2023-01-01 --end 2024-12-31
    """
    asyncio.run(_load_data_async(symbol, timeframe, mode, start_date, end_date))

async def _load_data_async(symbol, timeframe, mode, start_date, end_date):
    """Async implementation using new endpoint."""
    async with AsyncCLIClient() as client:
        # Call new endpoint
        response = await client.post(
            "/data/acquire/download",  # NEW endpoint
            json={
                "symbol": symbol,
                "timeframe": timeframe,
                "mode": mode,
                "start_date": start_date,
                "end_date": end_date,
            }
        )

        operation_id = response["operation_id"]
        console.print(f"[green]Download started: {operation_id}[/green]")

        # Poll for progress (existing logic)
        await _poll_operation_with_progress(operation_id, client)
```

**Files Created**:
- None (modify existing)

**Files Modified**:
- `ktrdr/api/endpoints/data.py` - Add new endpoint, deprecate old
- `ktrdr/api/dependencies.py` - Add get_acquisition_service()
- `ktrdr/api/models/data.py` - Add/update response models if needed
- `ktrdr/cli/data_commands.py` - Update load command

**Scope**:
1. Create POST /data/acquire/download endpoint
2. Deprecate POST /data/load (route to new endpoint with warning)
3. Add dependency injection for DataAcquisitionService
4. Update CLI `load` command to use new endpoint
5. Update CLI help text and examples
6. Add API tests for new endpoint
7. Add CLI tests for updated command
8. Update API documentation

**Integration Test Scenarios**:
- D3.1: Download via new API endpoint - verify operation tracking
- D3.2: Download via CLI - verify progress display
- D3.3: Use deprecated endpoint - verify warning logged

**Acceptance Criteria**:
- [ ] POST /data/acquire/download endpoint works
- [ ] POST /data/load shows deprecation warning
- [ ] CLI `ktrdr data load` uses new endpoint
- [ ] Operation tracking works end-to-end
- [ ] Progress updates display in CLI
- [ ] API tests pass (10+ tests)
- [ ] CLI tests pass (5+ tests)
- [ ] Integration tests D3.1-D3.3 pass
- [ ] API docs updated
- [ ] `make quality` passes

**Estimated Duration**: 2 days

---

### Phase 4 Summary

**Total Tasks**: 7 tasks (4.1-4.7)

**Total Duration**: 8-10 days (1.5-2 weeks)

**Breakdown**:
- Task 4.1: âœ… COMPLETE (0.5 days) - Shell created
- Task 4.2: âœ… COMPLETE (1 day) - Basic download flow
- Task 4.3: Gap Analysis Integration (2 days) - Mode-based gap detection
- Task 4.4: Segment Manager Integration (2-3 days) - Resilient downloads
- Task 4.5: DataLoadingOrchestrator Review (1 day) - Refactor or deprecate
- Task 4.6: Enhanced Progress Tracking (1-2 days) - Data-specific progress
- Task 4.7: API/CLI Wiring (2 days) - New endpoints

**Key Improvements in This Version**:
- âœ… SymbolCache dependency documented (Task 4.3)
- âœ… Mode parameter added to download_data signature (Task 4.3)
- âœ… Error handling patterns defined (Tasks 4.3, 4.4)
- âœ… Unit test specifications detailed (15-20+ tests per task)
- âœ… Configuration constants with env var overrides (Task 4.4)
- âœ… Data merging logic clarified (Task 4.4)
- âœ… Progress manager flow explained (Task 4.6)
- âœ… Task dependencies visualized (Phase 4 intro)
- âœ… Backward compatibility strategy (Phase 4 intro)
- âœ… Cancellation token flow documented (Task 4.4)

**Critical Notes**:
- See [Critical Success Factors](#critical-success-factors) for dependencies
- All configuration via environment variables (DATA_MAX_SEGMENT_SIZE, etc.)
- ServiceOrchestrator provides progress_manager and cancellation_token
- **IMPORTANT**: DataManager code stays intact during Phase 4 - deletion happens in Phase 5 only
- Tasks 4.3 â†’ 4.4 sequential, Task 4.6 can run in parallel

**Code Deletion Timeline**:
- **Phase 4**: Extract and copy code to DataAcquisitionService (DataManager STAYS intact)
- **Phase 5**: Delete DataManager entirely (~1500 LOC removed)
- See [Phase 5](#phase-5-cleanup--documentation) for complete deletion list

---

### Phase 4 Exit Criteria

**Components**:
- [x] DataAcquisitionService working
- [x] New API endpoints
- [x] New CLI commands

**Integration Tests**:
- [x] ALL Phase 0 tests pass via new endpoints

**Total Duration**: 1-2 weeks

### Quick Reference: Common Patterns

**1. Adding New Dependencies**:
```python
# In DataAcquisitionService.__init__()
from ktrdr.data.acquisition.gap_analyzer import GapAnalyzer
from ktrdr.data.components.symbol_cache import SymbolCache

self.gap_analyzer = GapAnalyzer()
self.symbol_cache = SymbolCache()
```

**2. Error Handling Pattern**:
```python
try:
    result = await some_operation()
except SpecificError as e:
    logger.warning(f"Operation failed: {e}")
    # Fall back to safe default
    result = safe_default_value
```

**3. Data Merging Pattern**:
```python
# Combine segments
combined = pd.concat(segments, ignore_index=False).sort_index()

# Merge with existing
if existing_data is not None and not existing_data.empty:
    merged = self.repository.merge_data(existing_data, combined)
else:
    merged = combined

self.repository.save_to_cache(symbol, timeframe, merged)
```

**4. Configuration Pattern**:
```python
class DataAcquisitionService:
    MAX_SEGMENT_SIZE = int(os.getenv("DATA_MAX_SEGMENT_SIZE", "5000"))

    def __init__(self):
        self.max_segment_size = self.MAX_SEGMENT_SIZE
```

**5. ServiceOrchestrator Integration**:
```python
# progress_manager and cancellation_token are provided by ServiceOrchestrator
# within the operation_func passed to start_managed_operation()

async def _download_operation():
    # These variables are available here:
    # - progress_manager (from ServiceOrchestrator)
    # - cancellation_token (from ServiceOrchestrator)

    await segment_manager.fetch_segments_with_resilience(
        progress_manager=progress_manager,  # Use provided manager
        cancellation_token=cancellation_token,  # Use provided token
    )
```

---

## Phase 5: Cleanup & Documentation

### Goal

Remove DataManager, deprecated code, finalize architecture.

### Context

**What Gets Deleted**:

After Phase 4 validates that DataAcquisitionService works correctly, Phase 5 removes the old implementation:

**Files to DELETE**:
- âœ‚ï¸ `ktrdr/data/data_manager.py` - **Entire file** (~1500 LOC)
- âœ‚ï¸ `ktrdr/data/data_manager_builder.py` - Builder pattern no longer needed
- âœ‚ï¸ `ktrdr/data/data_loading_orchestrator.py` - If Task 4.5 marked for deprecation

**Code to DELETE**:
- âœ‚ï¸ DataManager's `_fetch_head_timestamp_async()` - Now in DataAcquisitionService
- âœ‚ï¸ DataManager's `_validate_request_against_head_timestamp()` - Now in DataAcquisitionService
- âœ‚ï¸ DataManager's `_ensure_symbol_has_head_timestamp()` - Now in DataAcquisitionService
- âœ‚ï¸ DataManager's `_fetch_segments_with_component_async()` - Now in DataAcquisitionService
- âœ‚ï¸ DataManager's `_save_periodic_progress()` - Now in DataAcquisitionService
- âœ‚ï¸ DataManager's `_load_data_core_logic()` - Replaced by DataAcquisitionService.download_data()
- âœ‚ï¸ All other DataManager methods - **Everything goes**

**Endpoints to DELETE**:
- âœ‚ï¸ `POST /data/load` - Deprecated endpoint removed (users must use `/data/acquire/download`)

**Why Delete After Phase 4, Not During**:
- âœ… **Safety**: Can quickly rollback if issues discovered
- âœ… **Validation**: Gives time to thoroughly test new implementation
- âœ… **User migration**: Gives users warning period for deprecated endpoints
- âœ… **Confidence**: Only delete after 100% confidence in replacement

### Visual Summary: Code Evolution

**Phase 4 (Extraction)** - BOTH exist:
```
ktrdr/data/
â”œâ”€â”€ data_manager.py                    â† Still here! (~1500 LOC)
â”‚   â””â”€â”€ All original methods intact
â”œâ”€â”€ acquisition/
â”‚   â””â”€â”€ acquisition_service.py         â† NEW! Has extracted copies
â”‚       â”œâ”€â”€ download_data()            (extracted from load_data)
â”‚       â”œâ”€â”€ _fetch_head_timestamp()    (extracted from DataManager)
â”‚       â”œâ”€â”€ gap_analyzer               (extracted reference)
â”‚       â””â”€â”€ segment_manager            (extracted reference)
â””â”€â”€ repository/
    â””â”€â”€ data_repository.py             â† From Phase 2
```

**Phase 5 (Deletion)** - OLD removed:
```
ktrdr/data/
â”œâ”€â”€ âŒ data_manager.py                 â† DELETED!
â”œâ”€â”€ âŒ data_manager_builder.py         â† DELETED!
â”œâ”€â”€ acquisition/
â”‚   â””â”€â”€ acquisition_service.py         â† ONLY implementation now
â”‚       â””â”€â”€ (all methods stay)
â””â”€â”€ repository/
    â””â”€â”€ data_repository.py             â† Stays
```

### Duration

**3-4 days** (6 tasks)

### Tasks

**Task 5.1**: Delete DataManager and builder (0.5 day)
**Task 5.2**: Remove deprecated API endpoints (0.5 day)
**Task 5.3**: Remove DataLoadingOrchestrator if deprecated (0.5 day)
**Task 5.4**: Update all imports and references (1 day)
**Task 5.5**: Update documentation (1 day)
**Task 5.6**: Final validation and cleanup (0.5 day)

---

## Final Summary

### Total Timeline

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| 0 | Done âœ… | Test baseline |
| 1 | Done âœ… | Host service ready |
| 2 | 1 week | Repository extracted |
| 3 | 4-5 days | IB deleted |
| 4 | 1-2 weeks | Acquisition extracted |
| 5 | 3-4 days | Clean architecture |

**Total**: ~3-4 weeks

---

**Document Version**: 2.0 (Extraction-Focused)
**Created**: 2025-01-29
**Starting Point**: Commit 79f98d6 (End of Phase 1)
**Status**: PROPOSED - Ready for Review
