# Development Insights - Temporary Collection

⚠️ **TEMPORARY DOC** - Key learnings extracted from development session notes, to be integrated into proper documentation.

## IB Connection Management Insights

### Connection Stability Lessons
- **Avoid competing client connections**: Multiple IB client connections with different IDs cause instability and socket corruption
- **CLI command design trade-off**: Test commands that create separate connections are problematic - removed `test-head-timestamp` command for this reason
- **Connection reuse**: Prefer reusing existing connections over creating new ones for utilities

### Head Timestamp Fetching
- **Instrument-specific approaches**: Forex pairs may need different `whatToShow` options (TRADES, BID, ASK, MIDPOINT)
- **Fallback strategies**: Try multiple approaches when initial head timestamp fetch fails
- **Caching considerations**: Per-timeframe head timestamp caching can be useful but adds complexity

## Request Management Patterns

### Context-Aware Processing
- **Sequence matters**: Set request context BEFORE operations that need it
- **Example**: `set_request_context()` must be called before `check_proactive_pace_limit()`
- **Full context for deduplication**: Include date ranges in request keys, not just symbol+timeframe

### Pace Limiting Design Principles
- **Distinguish truly identical vs similar requests**: `AAPL:1h:2024-01-01:2024-01-02` vs `AAPL:1h:2024-01-03:2024-01-04`
- **Request key design**: Include all relevant parameters to avoid false positives
- **Debug visibility**: Provide clear reasons for waits/delays in logging

## Error Handling Architecture

### Classification Context
- **Use request metadata**: Error 162 meaning depends on request context (future date vs historical limit vs pace violation)
- **Context timing**: Set error handler context before making requests that might error
- **Fallback gracefully**: Provide reasonable defaults when context unavailable

### Validation Strategies
- **Adjust vs fail**: Consider adjusting invalid requests rather than failing (e.g., future dates → current date)
- **User experience**: Clear error messages explaining what was adjusted and why

## Async Operation Patterns

### Long-Running Tasks
- **Progress tracking**: Provide real-time progress updates for lengthy operations
- **Cancellation support**: Implement graceful cancellation for user-initiated stops
- **Job management**: Track operation status and allow monitoring

### CLI Integration
- **Progress UX**: Rich progress displays improve user experience
- **Command availability**: Some features are implemented (AsyncDataLoader) while others are planned (CLI commands)

## Data Loading Architecture

### Segmentation Strategy
- **Mode-aware processing**: Different strategies for `tail` vs `backfill` mode
- **Gap analysis**: Skip micro-gap analysis for historical backfill to avoid thousands of tiny segments
- **Large segment preference**: For backfill, prefer fewer large segments over many small ones

### Implementation Reality Check
- **Document accuracy**: Fix summaries often describe planned vs actual implementation
- **File path accuracy**: Code moves between modules - `data/` vs `ib/` organization
- **Command evolution**: CLI commands get added, removed, and restructured

---

## Integration Tasks

These insights should be integrated into:

1. **IB Integration Guide** - Connection management and head timestamp lessons
2. **Error Handling Documentation** - Context-aware classification patterns  
3. **CLI Development Guidelines** - Command design trade-offs and UX patterns
4. **Data Loading Architecture** - Segmentation and async operation patterns
5. **Development Best Practices** - Context sequencing and debugging visibility

## Neural Network Architecture Insights

### Configuration Coupling Problems
- **Avoid implementation coupling**: Strategy YAML files should not control low-level PyTorch implementation details
- **Configuration boundaries**: Strategy configs should control business logic only (hidden layers, dropout, learning rate)
- **Implementation details should be hardcoded**: Output activations, loss functions should be determined by model task type, not configuration
- **Example problem**: `output_activation: "softmax"` in strategy configs led to double-softmax bugs

### Separation of Concerns in Neural Models
- **Model+Trainer coupling**: ModelTrainer should own both model architecture AND loss function selection
- **Task-based architecture**: Use ModelTask enum (CLASSIFICATION, REGRESSION) to determine implementation details
- **Consistent output format**: Classification models should ALWAYS output raw logits (no configuration options)
- **Interface contracts**: Explicit validation between model output format and inference expectations

### Component Design Principles
- **Tight coupling where needed**: Model and trainer must work together - don't over-abstract
- **Clean interfaces elsewhere**: Clear boundaries between business logic and implementation
- **Validation and contracts**: Explicit checks that components work together correctly
- **PyTorch conventions**: Follow framework conventions strictly rather than allowing configuration flexibility

## Training Pipeline Architecture Insights

### Separation of Concerns - Training vs Indicators
- **Training pipeline should be indicator-agnostic**: Should only process indicators declared in strategy config
- **Avoid hard-coded calculations**: All mathematical calculations belong in indicator classes, not training pipeline
- **Component responsibilities are clear**:
  - ✅ Training: Load config, orchestrate engines, feature engineering, neural training
  - ❌ Training: Direct indicator calculations, derived metrics, mathematical transformations
  - ✅ Indicators: All mathematical calculations, derived metrics, validation, error handling

### Architecture Violation Patterns
- **Problem**: Adding hard-coded indicator calculations to training pipeline
- **Result**: Training fails when strategy doesn't use those specific indicators
- **Solution**: Create proper indicator classes instead of embedding calculations in training
- **Prevention**: Establish clear architectural boundaries and guidelines

### Indicator Development Patterns  
- **Follow established patterns**: BaseIndicator inheritance, factory registration, schema validation
- **Automatic integration**: Proper indicators automatically work with API, CLI, strategies
- **Proper error handling**: Use DataError for indicator calculation failures
- **Standardized outputs**: Consistent naming and format across all indicators

## Cleanup Tasks

Original fix summary files to remove after integration:
- `HEAD_TIMESTAMP_FIX_SUMMARY.md` 
- `PACE_LIMITING_FIX_SUMMARY.md`
- `DATA_LOADING_IMPROVEMENTS.md` (already moved but needs integration)
- `NEURAL_ARCHITECTURE_FIXES.md` ✅ (insights extracted, removed)
- `CLAUDE-training-fix-plan.md` (extract insights then remove)