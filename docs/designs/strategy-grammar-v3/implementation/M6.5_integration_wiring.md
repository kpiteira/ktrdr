---
design: ../DESIGN.md
architecture: ../ARCHITECTURE.md
---

# Milestone 6.5: V3 Integration Wiring

**Branch:** `feature/strategy-grammar-v3-m6` (continues from M6)
**Prerequisite:** M1-M6 complete
**Builds on:** M4 TrainingPipelineV3, M5 FeatureCacheV3

## Goal

Wire the v3 components (TrainingPipelineV3, FeatureCacheV3, ModelMetadataV3) into the actual execution flows so that v3 strategies work end-to-end through the CLI and API.

## Why This Milestone

M4 and M5 created the v3 pipeline classes but never wired them into the execution flows:

- `LocalTrainingOrchestrator._execute_training()` always calls v2 `TrainingPipeline.train_strategy()`
- Backtest execution doesn't use `FeatureCacheV3` or load `ModelMetadataV3`
- The v3 components exist but are never invoked during actual training/backtest operations

This milestone closes that gap with minimal, focused changes.

---

## Tasks

### Task 6.5.1: Wire V3 Training in LocalTrainingOrchestrator

**File(s):** `ktrdr/api/services/training/local_orchestrator.py`
**Type:** CODING
**Estimated scope:** ~50 lines

**Description:**
Modify `_execute_training()` to detect v3 format and use `TrainingPipelineV3` instead of `TrainingPipeline.train_strategy()`.

**Implementation Notes:**

The orchestrator already has `_is_v3_format()` and `_save_v3_metadata()` static methods from M4. Now we need to use them:

```python
def _execute_training(self) -> dict[str, Any]:
    # ... existing loading code ...
    config = self._load_strategy_config(self._context.strategy_path)

    if self._is_v3_format(config):
        result = self._execute_v3_training(config)
    else:
        result = self._execute_v2_training(config)

    # ... existing session_info code ...
    return result

def _execute_v3_training(self, config: dict[str, Any]) -> dict[str, Any]:
    """Execute training using TrainingPipelineV3."""
    from ktrdr.config.feature_resolver import FeatureResolver
    from ktrdr.config.strategy_loader import StrategyConfigurationLoader
    from ktrdr.training.training_pipeline import TrainingPipelineV3

    # Load as typed v3 config
    loader = StrategyConfigurationLoader()
    v3_config = loader.load_v3_strategy(self._context.strategy_path)

    # Resolve features (needed for metadata)
    resolver = FeatureResolver()
    resolved = resolver.resolve(v3_config)
    resolved_features = [f.feature_id for f in resolved]

    # Create and run pipeline
    pipeline = TrainingPipelineV3(v3_config)
    # ... load data, prepare features, train model ...

    # Save v3 metadata
    self._save_v3_metadata(
        model_path=result["model_path"],
        config=config,
        resolved_features=resolved_features,
        training_metrics=result.get("training_metrics", {}),
        training_symbols=self._context.symbols,
        training_timeframes=self._context.timeframes,
    )

    return result

def _execute_v2_training(self, config: dict[str, Any]) -> dict[str, Any]:
    """Execute training using legacy TrainingPipeline (existing code)."""
    # Move existing TrainingPipeline.train_strategy() call here
    ...
```

**Key Changes:**
1. Extract existing v2 training code into `_execute_v2_training()`
2. Add `_execute_v3_training()` that uses `TrainingPipelineV3`
3. Branch in `_execute_training()` based on `_is_v3_format()`
4. Call `_save_v3_metadata()` after v3 training completes

**Testing Requirements:**

*Unit Tests:* `tests/unit/training/test_local_orchestrator_v3.py`
- [ ] V3 format detected correctly
- [ ] V3 training uses TrainingPipelineV3
- [ ] V2 training still uses TrainingPipeline (backward compat)
- [ ] ModelMetadataV3 saved after v3 training

**Acceptance Criteria:**
- [ ] `ktrdr models train` with v3 strategy uses TrainingPipelineV3
- [ ] `metadata_v3.json` created in model directory
- [ ] V2 strategies still work (no regression)

---

### Task 6.5.2: Wire V3 Backtest in BacktestingService

**File(s):** `ktrdr/backtesting/backtesting_service.py`, `ktrdr/api/services/backtest_service.py`
**Type:** CODING
**Estimated scope:** ~40 lines

**Description:**
Update backtest execution to detect v3 models and use `FeatureCacheV3` with `ModelMetadataV3`.

**Implementation Notes:**

BacktestingService already has v3 static methods from M5:
- `is_v3_model(model_path)` - checks for metadata_v3.json
- `load_v3_metadata(model_path)` - loads ModelMetadataV3
- `reconstruct_config_from_metadata(metadata)` - rebuilds StrategyConfigurationV3

Wire these into the backtest flow:

```python
async def run_backtest(self, model_path: Path, ...):
    if self.is_v3_model(model_path):
        return await self._run_v3_backtest(model_path, ...)
    else:
        return await self._run_v2_backtest(model_path, ...)

async def _run_v3_backtest(self, model_path: Path, ...) -> BacktestResult:
    """Run backtest using v3 feature generation."""
    # Load v3 metadata
    metadata = self.load_v3_metadata(model_path)

    # Reconstruct config
    config = self.reconstruct_config_from_metadata(metadata)

    # Create v3 feature cache
    cache = FeatureCacheV3(config, metadata)

    # Load data, compute features, run inference
    data = await self._load_data(...)
    features = cache.compute_features(data)

    # Run model inference
    ...
```

**Testing Requirements:**

*Unit Tests:* `tests/unit/backtesting/test_backtesting_service_v3_wiring.py`
- [ ] V3 model detected by metadata_v3.json presence
- [ ] V3 backtest uses FeatureCacheV3
- [ ] V2 models still work (backward compat)
- [ ] Feature order validated against resolved_features

**Acceptance Criteria:**
- [ ] Backtest with v3-trained model uses FeatureCacheV3
- [ ] Feature mismatch produces clear error
- [ ] V2 models still backtest correctly

---

### Task 6.5.3: E2E Test - Train and Backtest V3 Strategy

**File(s):** `tests/e2e/test_v3_train_backtest.py` (NEW)
**Type:** CODING
**Estimated scope:** ~100 lines

**Description:**
Create true end-to-end test that:
1. Starts with a v3 strategy YAML
2. Runs training via CLI/API
3. Verifies model has metadata_v3.json
4. Runs backtest on trained model
5. Verifies features match training

**Implementation Notes:**

```python
@pytest.mark.e2e
class TestV3TrainBacktest:
    """End-to-end test for v3 strategy train → backtest flow."""

    def test_train_creates_v3_metadata(self, v3_strategy_path, tmp_model_dir):
        """Training v3 strategy creates metadata_v3.json."""
        # Run training
        result = subprocess.run([
            "uv", "run", "ktrdr", "models", "train",
            str(v3_strategy_path),
            "--output", str(tmp_model_dir),
            "--epochs", "1"
        ], capture_output=True, text=True)

        assert result.returncode == 0
        assert (tmp_model_dir / "metadata_v3.json").exists()

        # Verify metadata contents
        with open(tmp_model_dir / "metadata_v3.json") as f:
            metadata = json.load(f)

        assert metadata["strategy_version"] == "3.0"
        assert len(metadata["resolved_features"]) > 0

    def test_backtest_uses_v3_features(self, trained_v3_model):
        """Backtest on v3 model uses FeatureCacheV3."""
        # Run backtest
        result = subprocess.run([
            "uv", "run", "ktrdr", "backtest",
            str(trained_v3_model),
            "--symbol", "EURUSD",
            "--start", "2024-01-01",
            "--end", "2024-01-31"
        ], capture_output=True, text=True)

        assert result.returncode == 0
        # Verify no feature mismatch errors
        assert "feature mismatch" not in result.stderr.lower()
```

**Testing Requirements:**

This IS the test - it validates the full flow works.

**Acceptance Criteria:**
- [ ] Test trains v3 strategy successfully
- [ ] Test verifies metadata_v3.json created
- [ ] Test runs backtest on trained model
- [ ] Test passes in CI

---

### Task 6.5.4: Update Handoff Documentation

**File(s):** `docs/designs/strategy-grammar-v3/implementation/HANDOFF_M6.md`
**Type:** DOCUMENTATION

**Description:**
Update HANDOFF_M6.md to document the wiring work completed in M6.5.

**Changes:**
- Document the integration wiring pattern
- Note any gotchas discovered during implementation
- Update "V3 Training Integration Gap" section to show it's resolved
- Add E2E test results

---

## E2E Test Scenario

**Purpose:** Prove v3 strategies work end-to-end through actual execution paths
**Duration:** ~5-10 minutes (full training, cannot limit epochs via CLI)
**Prerequisites:** Docker running, test data available in `~/.ktrdr/shared/data/`

### Required Data Files

The v3 test strategy uses these symbols at 1h timeframe:
- `~/.ktrdr/shared/data/EURUSD_1h.csv`
- `~/.ktrdr/shared/data/GBPUSD_1h.csv`
- `~/.ktrdr/shared/data/AUDUSD_1h.csv`

### Test Strategy Setup

Before running the E2E test, create a minimal v3 test strategy with reduced epochs:

```bash
# Create test strategy with minimal epochs for faster E2E testing
cat > /tmp/v3_e2e_test.yaml << 'EOF'
name: v3_e2e_test
description: Minimal v3 strategy for E2E testing
version: '3.0'
scope: universal

training_data:
  symbols:
    mode: single
    symbol: EURUSD
  timeframes:
    mode: single
    list: [1h]
    base_timeframe: 1h
  history_required: 100

indicators:
  rsi_14:
    type: RSI
    period: 14
    source: close

fuzzy_sets:
  rsi_14:
    oversold:
      type: triangular
      parameters: [0, 20, 40]
    neutral:
      type: triangular
      parameters: [30, 50, 70]
    overbought:
      type: triangular
      parameters: [60, 80, 100]
    indicator: rsi_14

nn_inputs:
  - fuzzy_set: rsi_14
    timeframes: all

model:
  type: mlp
  architecture:
    hidden_layers: [16, 8]
    activation: relu
    output_activation: softmax
    dropout: 0.1
  features:
    include_price_context: false
    lookback_periods: 1
    scale_features: true
  training:
    learning_rate: 0.001
    batch_size: 32
    epochs: 3  # Minimal for testing
    optimizer: adam
    early_stopping:
      enabled: false

decisions:
  output_format: classification
  confidence_threshold: 0.6
  position_awareness: false

training:
  method: supervised
  labels:
    source: zigzag
    zigzag_threshold: 0.02
    label_lookahead: 10
  data_split:
    train: 0.7
    validation: 0.15
    test: 0.15
EOF
```

### Test Steps

```bash
#!/bin/bash
# M6.5 E2E Test: V3 Integration Wiring

set -e

echo "=== M6.5 E2E Test: V3 Integration Wiring ==="

# Setup
V3_STRATEGY="/tmp/v3_e2e_test.yaml"
MODEL_DIR="/tmp/m6.5_test_models"
STRATEGY_NAME="v3_e2e_test"

# Clean up from previous runs
rm -rf "$MODEL_DIR"
mkdir -p "$MODEL_DIR"

# Create test strategy (run the cat command above first)
if [ ! -f "$V3_STRATEGY" ]; then
    echo "FAIL: Test strategy not found. Create it first."
    exit 1
fi

# Test 1: V3 dry-run shows features
echo "Test 1: V3 dry-run..."
DRY_RUN_OUTPUT=$(uv run ktrdr models train "$V3_STRATEGY" \
    --start-date 2024-01-01 --end-date 2024-03-01 \
    --dry-run 2>&1)

if echo "$DRY_RUN_OUTPUT" | grep -q "rsi_14"; then
    echo "Test 1: PASS - Dry-run shows indicators"
else
    echo "FAIL: Dry-run output missing indicators"
    echo "$DRY_RUN_OUTPUT"
    exit 1
fi

# Test 2: Train v3 strategy
echo "Test 2: Training v3 strategy..."
uv run ktrdr models train "$V3_STRATEGY" \
    --start-date 2024-01-01 --end-date 2024-06-01 \
    --models-dir "$MODEL_DIR"

# Find the created model directory (named after strategy)
MODEL_PATH=$(find "$MODEL_DIR" -type d -name "${STRATEGY_NAME}*" | head -1)
if [ -z "$MODEL_PATH" ]; then
    echo "FAIL: Model directory not created"
    ls -la "$MODEL_DIR"
    exit 1
fi
echo "Test 2: PASS - Training completed, model at $MODEL_PATH"

# Test 3: Verify metadata_v3.json created
echo "Test 3: Checking metadata_v3.json..."
if [ ! -f "$MODEL_PATH/metadata_v3.json" ]; then
    echo "FAIL: metadata_v3.json not created"
    echo "Contents of model directory:"
    ls -la "$MODEL_PATH"
    exit 1
fi

# Verify metadata contents
uv run python << EOF
import json
import sys

model_path = "$MODEL_PATH"
with open(f"{model_path}/metadata_v3.json") as f:
    meta = json.load(f)

errors = []
if meta.get("strategy_version") != "3.0":
    errors.append(f"Wrong version: {meta.get('strategy_version')}")
if not meta.get("resolved_features"):
    errors.append("No resolved_features")
if "indicators" not in meta:
    errors.append("Missing indicators")
if "fuzzy_sets" not in meta:
    errors.append("Missing fuzzy_sets")
if "nn_inputs" not in meta:
    errors.append("Missing nn_inputs")

if errors:
    print("FAIL: Metadata validation errors:")
    for e in errors:
        print(f"  - {e}")
    sys.exit(1)

print(f"Metadata valid: {len(meta['resolved_features'])} features")
print(f"  Indicators: {list(meta['indicators'].keys())}")
print(f"  Fuzzy sets: {list(meta['fuzzy_sets'].keys())}")
EOF
echo "Test 3: PASS - metadata_v3.json valid"

# Test 4: Run backtest on v3 model
echo "Test 4: Running backtest..."
uv run ktrdr backtest run "$STRATEGY_NAME" EURUSD 1h \
    --start-date 2024-06-01 --end-date 2024-07-01 \
    --model-path "$MODEL_PATH" \
    --verbose

echo "Test 4: PASS - Backtest completed"

# Test 5: Verify v2 regression (optional - if v2 strategy exists)
echo "Test 5: V2 regression check..."
V2_STRATEGY="strategies/neuro_mean_reversion.yaml"
if [ -f "$V2_STRATEGY" ]; then
    uv run ktrdr models train "$V2_STRATEGY" \
        --start-date 2024-01-01 --end-date 2024-03-01 \
        --dry-run > /dev/null 2>&1
    echo "Test 5: PASS - V2 still works"
else
    echo "Test 5: SKIP - No v2 strategy available"
fi

# Cleanup
rm -rf "$MODEL_DIR"
rm -f "$V3_STRATEGY"

echo ""
echo "=== M6.5 E2E Test: ALL PASSED ==="
```

### Success Criteria

**Critical (proves wiring works):**
- [ ] `metadata_v3.json` created in model directory - this ONLY happens if `_save_v3_metadata()` is called, which requires the v3 training path to be wired
- [ ] `metadata_v3.json` contains `resolved_features` list - proves FeatureResolver was used
- [ ] Backtest completes without "feature mismatch" errors - proves FeatureCacheV3 is being used with correct feature order

**Functional:**
- [ ] V3 strategy trains without "No indicators" or format errors
- [ ] Dry-run shows v3 indicators and fuzzy sets
- [ ] V2 strategies still work (regression test)

**What would fail if wiring is NOT done:**
- Without Task 6.5.1: Training would use v2 TrainingPipeline, `metadata_v3.json` would NOT be created
- Without Task 6.5.2: Backtest would use v2 FeatureCache, likely causing feature mismatch errors or wrong predictions

---

## Completion Checklist

- [ ] Task 6.5.1: V3 training wired in LocalTrainingOrchestrator
- [ ] Task 6.5.2: V3 backtest wired in BacktestingService
- [ ] Task 6.5.3: E2E test created and passing
- [ ] Task 6.5.4: Handoff documentation updated
- [ ] All unit tests pass: `make test-unit`
- [ ] E2E test script passes
- [ ] M1-M6 tests still pass (no regression)
- [ ] Quality gates pass: `make quality`
- [ ] Code reviewed and merged

---

## Risk Areas

| Risk | Mitigation |
|------|------------|
| Breaking v2 training | Keep v2 path unchanged, branch early based on format |
| TrainingPipelineV3 interface mismatch | V3 pipeline tested in isolation in M4, just wire it |
| Missing dependencies in production | E2E test catches import/runtime issues |
| Data format differences | FeatureCacheV3 uses same data loading, just different feature generation |

---

## Root Cause Analysis: Why This Milestone Was Needed

This section documents how the integration gap occurred, as a learning exercise for future milestone planning.

### The Pattern Evolution

| Milestone | Component | Design Decision | Impact |
|-----------|-----------|-----------------|--------|
| **M2** | IndicatorEngine | Added `_indicators` dict **alongside** `indicators` list in **same class** | ✅ Callers unchanged, mode detection internal |
| **M3** | FuzzyEngine | Added `_fuzzy_sets` dict **alongside** `_membership_functions` in **same class** | ✅ Callers unchanged, mode detection internal |
| **M4** | TrainingPipeline | Created **new class** `TrainingPipelineV3` | ⚠️ **Pattern break** - callers must change |
| **M5** | FeatureCache | Created **new class** `FeatureCacheV3` | ⚠️ **Same issue** - callers must change |

**Key insight:** M2/M3 used **internal mode-switching** - the same class handles both v2 and v3. Callers don't need to change because `IndicatorEngine(v3_dict)` and `IndicatorEngine(v2_list)` both work transparently.

But M4/M5 created **separate classes**, which means **someone has to decide which class to instantiate**. That "someone" is the orchestrator - and that wiring was never done.

### What the Original Design Said

From M4_training_pipeline.md Task 4.4:
```python
# The design IMPLIED TrainingPipeline would be modified to handle v3:
pipeline = TrainingPipeline(config)  # Same class, would detect v3 internally
pipeline.train(...)
```

### What Was Actually Implemented

From HANDOFF_M4.md Task 4.1:
> "**TrainingPipelineV3 is a separate class (not modification of TrainingPipeline)**"
> "Pattern: separate class maintains backward compatibility"

From HANDOFF_M4.md Task 4.4:
> "**Integration with training flow is separate**"
> "Full integration (calling these after training) is straightforward **but wasn't done to avoid modifying complex training flow**"

### Why This Happened

1. **Emergent decision during implementation**: The decision to create a separate class was made during Task 4.1 implementation, not in the original design

2. **Task scope interpretation**: Task 4.4 said "Update training worker to pass v3 config" - interpreted as "add utility methods" rather than "wire the new class into the execution flow"

3. **Complexity avoidance**: The handoff explicitly states wiring "wasn't done to avoid modifying complex training flow"

4. **Deferred without tracking**: Work was noted as "Future: Full V3 Training Execution" but never captured as a blocking task

### E2E Test Gap

The M4 E2E test tested components in isolation:
```bash
# This tests TrainingPipelineV3 directly via Python script
uv run python -c "
from ktrdr.training.training_pipeline import TrainingPipelineV3
pipeline = TrainingPipelineV3(config)
features = pipeline.prepare_features(data)
"
```

But it never tested the **actual execution path**:
```
CLI → API → LocalTrainingOrchestrator._execute_training() → TrainingPipeline.train_strategy()
                                                            ↑
                                                    Always calls v2!
```

### Lessons Learned

1. **Design pattern consistency**: If earlier components use internal mode-switching (M2/M3), later components should follow the same pattern OR the design should explicitly document:
   - Why the pattern is changing
   - What wiring is needed as a result
   - Which task owns the wiring work

2. **Task scope clarity**: Ambiguous task descriptions lead to ambiguous implementations:
   - ❌ "Update X to support v3" - unclear if this means wiring
   - ✅ "Wire v3 execution path in X" - explicitly about integration
   - ✅ "Add v3 mode detection and dispatch in X" - clear scope

3. **E2E tests must test actual paths**: Integration tests should exercise the real user-facing flow:
   - ❌ Testing `TrainingPipelineV3` class directly
   - ✅ Testing `ktrdr models train strategy.yaml` and verifying `metadata_v3.json` exists

4. **"Deferred work" needs explicit tracking**: If integration is deferred, create a blocking task immediately:
   - Add it to the current milestone as a dependency
   - Or create a follow-up milestone (like this M6.5)
   - Never leave it as just a "Future work" note in handoff docs

5. **Separate classes require wiring tasks**: When creating `FooV3` instead of modifying `Foo`:
   - Add explicit task: "Update callers to detect v3 and use FooV3"
   - The acceptance criteria must include "v3 path exercised in E2E test"
